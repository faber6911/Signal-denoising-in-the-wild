{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import copy\n",
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if colab:\n",
    "  !pip install -q livelossplot\n",
    "  !pip install -q kaldi_io\n",
    "  !pip install -q kaldiio\n",
    "  os.environ['KALDI_ROOT'] = '/content/drive/My Drive/Stage-Imaging/Signal-denoising-in-the-wild'\n",
    "else:\n",
    "    os.environ['KALDI_ROOT'] = '/opt/kaldi/'\n",
    "    \n",
    "import kaldi_io\n",
    "import kaldiio\n",
    "#from livelossplot import PlotLosses\n",
    "import librosa\n",
    "from tqdm.notebook import tqdm\n",
    "#from pesq import pesq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import SequenceDataset\n",
    "from utils import ScheduledOptim, change_path_scp, EnergyConservingLoss, plot_modelPerformance, l1_mse_loss, splitAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  device\n",
    "else:  \n",
    "  os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 25334 samples with at most 247 samples for one class\n",
      "Totally 5482 samples with at most 247 samples for one class\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SequenceDataset('../data/train/trainTOTAL.scp', '../data/utt2spkTOTAL.scp', min_length = 16000, colab = False)\n",
    "test_dataset = SequenceDataset('../data/test/testTOTAL.scp', '../data/utt2spkTOTAL.scp', min_length = 16000, colab = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_dataset, batch_size = 1, shuffle=True)\n",
    "test_data = DataLoader(test_dataset, batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean, noisy, _ = next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.squeeze().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stft(noisy.squeeze(), n_fft = 255, win_length = 255,# window = torch.hann_window(window_length = 255),\n",
    "          hop_length=125).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9d186422b0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(new[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, activation=True, batch_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.activation = activation\n",
    "        self.lrelu = torch.nn.LeakyReLU(0.2, True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation:\n",
    "            out = self.conv(self.lrelu(x))\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            return self.bn(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class DeconvBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, batch_norm=True, dropout=False):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.deconv = torch.nn.ConvTranspose2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "        self.drop = torch.nn.Dropout(0.5)\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.deconv(self.relu(x)))\n",
    "        else:\n",
    "            out = self.deconv(self.relu(x))\n",
    "\n",
    "        if self.dropout:\n",
    "            return self.drop(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class Generator128(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Generator128, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, activation=False, batch_norm=False)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4)\n",
    "        self.conv4 = ConvBlock(num_filter * 4, num_filter * 8)\n",
    "        self.conv5 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv6 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv7 = ConvBlock(num_filter * 8, num_filter * 8, batch_norm=False)\n",
    "        # Decoder\n",
    "        self.deconv1 = DeconvBlock(num_filter * 8, num_filter * 8, dropout=True)\n",
    "        self.deconv2 = DeconvBlock(num_filter * 8 * 2, num_filter * 8, dropout=True)\n",
    "        self.deconv3 = DeconvBlock(num_filter * 8 * 2, num_filter * 8, dropout=True)\n",
    "        self.deconv4 = DeconvBlock(num_filter * 8 * 2, num_filter * 4)\n",
    "        self.deconv5 = DeconvBlock(num_filter * 4 * 2, num_filter * 2)\n",
    "        self.deconv6 = DeconvBlock(num_filter * 2 * 2, num_filter)\n",
    "        self.deconv7 = DeconvBlock(num_filter * 2, output_dim, batch_norm=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.conv1(x)\n",
    "        enc2 = self.conv2(enc1)\n",
    "        enc3 = self.conv3(enc2)\n",
    "        enc4 = self.conv4(enc3)\n",
    "        enc5 = self.conv5(enc4)\n",
    "        enc6 = self.conv6(enc5)\n",
    "        enc7 = self.conv7(enc6)\n",
    "        # Decoder with skip-connections\n",
    "        dec1 = self.deconv1(enc7)\n",
    "        dec1 = torch.cat([dec1, enc6], 1)\n",
    "        dec2 = self.deconv2(dec1)\n",
    "        dec2 = torch.cat([dec2, enc5], 1)\n",
    "        dec3 = self.deconv3(dec2)\n",
    "        dec3 = torch.cat([dec3, enc4], 1)\n",
    "        dec4 = self.deconv4(dec3)\n",
    "        dec4 = torch.cat([dec4, enc3], 1)\n",
    "        dec5 = self.deconv5(dec4)\n",
    "        dec5 = torch.cat([dec5, enc2], 1)\n",
    "        dec6 = self.deconv6(dec5)\n",
    "        dec6 = torch.cat([dec6, enc1], 1)\n",
    "        dec7 = self.deconv7(dec6)\n",
    "        out = torch.nn.Tanh()(dec7)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)\n",
    "            if isinstance(m, DeconvBlock):\n",
    "                torch.nn.init.normal(m.deconv.weight, mean, std)\n",
    "                \n",
    "class Discriminator128(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Discriminator128, self).__init__()\n",
    "\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, activation=False, batch_norm=False)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4, stride=1)\n",
    "        self.conv4 = ConvBlock(num_filter * 4, output_dim, stride=1, batch_norm=False)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = torch.cat([x, label], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        out = torch.nn.Sigmoid()(x)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)\n",
    "                \n",
    "                \n",
    "# Plot losses\n",
    "def plot_loss(d_losses, g_losses, num_epochs, save=False, save_dir='results/', show=True):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, num_epochs)\n",
    "    ax.set_ylim(0, max(np.max(g_losses), np.max(d_losses))*1.1)\n",
    "    plt.xlabel('# of Epochs')\n",
    "    plt.ylabel('Loss values')\n",
    "    plt.plot(d_losses, label='Discriminator')\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.legend()\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'Loss_values_epoch_{:d}'.format(num_epochs) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-2b636c1a2bbf>:94: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.conv.weight, mean, std)\n",
      "<ipython-input-13-2b636c1a2bbf>:96: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.deconv.weight, mean, std)\n",
      "<ipython-input-13-2b636c1a2bbf>:119: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.conv.weight, mean, std)\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "G = Generator128(2, 128, 2)\n",
    "D = Discriminator128(4, 128, 1)\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "G.normal_weight_init(mean=0.0, std=0.02)\n",
    "D.normal_weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "# Set the logger\n",
    "# D_log_dir = save_dir + 'D_logs'\n",
    "# G_log_dir = save_dir + 'G_logs'\n",
    "# if not os.path.exists(D_log_dir):\n",
    "#     os.mkdir(D_log_dir)\n",
    "# D_logger = Logger(D_log_dir)\n",
    "\n",
    "# if not os.path.exists(G_log_dir):\n",
    "#     os.mkdir(G_log_dir)\n",
    "# G_logger = Logger(G_log_dir)\n",
    "\n",
    "# Loss function\n",
    "BCE_loss = torch.nn.BCELoss().cuda()\n",
    "L1_loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "# Optimizers\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training GAN\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/25334], D_loss: 0.7294, G_loss: 25.7622\n",
      "Epoch [1/5], Step [2/25334], D_loss: 0.7412, G_loss: 9.2083\n",
      "Epoch [1/5], Step [3/25334], D_loss: 0.6758, G_loss: 13.8910\n",
      "Epoch [1/5], Step [4/25334], D_loss: 0.7397, G_loss: 7.8289\n",
      "Epoch [1/5], Step [5/25334], D_loss: 0.6928, G_loss: 17.3188\n",
      "Epoch [1/5], Step [6/25334], D_loss: 0.6241, G_loss: 9.8831\n",
      "Epoch [1/5], Step [7/25334], D_loss: 0.7246, G_loss: 12.0516\n",
      "Epoch [1/5], Step [8/25334], D_loss: 0.6421, G_loss: 11.9514\n",
      "Epoch [1/5], Step [9/25334], D_loss: 0.6862, G_loss: 23.6577\n",
      "Epoch [1/5], Step [10/25334], D_loss: 0.7436, G_loss: 6.7836\n",
      "Epoch [1/5], Step [11/25334], D_loss: 0.6735, G_loss: 10.0622\n",
      "Epoch [1/5], Step [12/25334], D_loss: 0.7208, G_loss: 6.2098\n",
      "Epoch [1/5], Step [13/25334], D_loss: 0.7098, G_loss: 5.1032\n",
      "Epoch [1/5], Step [14/25334], D_loss: 0.6948, G_loss: 8.6789\n",
      "Epoch [1/5], Step [15/25334], D_loss: 0.6662, G_loss: 6.3705\n",
      "Epoch [1/5], Step [16/25334], D_loss: 0.9438, G_loss: 5.0320\n",
      "Epoch [1/5], Step [17/25334], D_loss: 0.7688, G_loss: 17.3972\n",
      "Epoch [1/5], Step [18/25334], D_loss: 0.7013, G_loss: 11.2618\n",
      "Epoch [1/5], Step [19/25334], D_loss: 0.7208, G_loss: 10.6336\n",
      "Epoch [1/5], Step [20/25334], D_loss: 0.6314, G_loss: 7.9970\n",
      "Epoch [1/5], Step [21/25334], D_loss: 0.7406, G_loss: 4.9324\n",
      "Epoch [1/5], Step [22/25334], D_loss: 0.6816, G_loss: 61.1920\n",
      "Epoch [1/5], Step [23/25334], D_loss: 0.6636, G_loss: 8.2496\n",
      "Epoch [1/5], Step [24/25334], D_loss: 0.6431, G_loss: 15.0289\n",
      "Epoch [1/5], Step [25/25334], D_loss: 0.7750, G_loss: 4.1833\n",
      "Epoch [1/5], Step [26/25334], D_loss: 0.6248, G_loss: 17.3638\n",
      "Epoch [1/5], Step [27/25334], D_loss: 0.8420, G_loss: 5.3633\n",
      "Epoch [1/5], Step [28/25334], D_loss: 0.7555, G_loss: 6.0930\n",
      "Epoch [1/5], Step [29/25334], D_loss: 0.6782, G_loss: 12.2595\n",
      "Epoch [1/5], Step [30/25334], D_loss: 0.5965, G_loss: 11.6862\n",
      "Epoch [1/5], Step [31/25334], D_loss: 0.6583, G_loss: 20.0321\n",
      "Epoch [1/5], Step [32/25334], D_loss: 0.6572, G_loss: 28.9162\n",
      "Epoch [1/5], Step [33/25334], D_loss: 0.6374, G_loss: 9.6072\n",
      "Epoch [1/5], Step [34/25334], D_loss: 0.7272, G_loss: 42.4162\n",
      "Epoch [1/5], Step [35/25334], D_loss: 0.6735, G_loss: 9.0513\n",
      "Epoch [1/5], Step [36/25334], D_loss: 0.5708, G_loss: 15.8127\n",
      "Epoch [1/5], Step [37/25334], D_loss: 0.5387, G_loss: 26.3735\n",
      "Epoch [1/5], Step [38/25334], D_loss: 0.7427, G_loss: 3.9971\n",
      "Epoch [1/5], Step [39/25334], D_loss: 0.6745, G_loss: 8.1969\n",
      "Epoch [1/5], Step [40/25334], D_loss: 0.5677, G_loss: 8.8669\n",
      "Epoch [1/5], Step [41/25334], D_loss: 0.5945, G_loss: 9.2680\n",
      "Epoch [1/5], Step [42/25334], D_loss: 0.6117, G_loss: 6.3355\n",
      "Epoch [1/5], Step [43/25334], D_loss: 0.7111, G_loss: 4.4881\n",
      "Epoch [1/5], Step [44/25334], D_loss: 0.8565, G_loss: 6.5964\n",
      "Epoch [1/5], Step [45/25334], D_loss: 0.5681, G_loss: 12.2586\n",
      "Epoch [1/5], Step [46/25334], D_loss: 0.5811, G_loss: 9.6905\n",
      "Epoch [1/5], Step [47/25334], D_loss: 0.7175, G_loss: 6.6916\n",
      "Epoch [1/5], Step [48/25334], D_loss: 0.4873, G_loss: 8.5029\n",
      "Epoch [1/5], Step [49/25334], D_loss: 0.5835, G_loss: 6.4652\n",
      "Epoch [1/5], Step [50/25334], D_loss: 0.8109, G_loss: 3.1928\n",
      "Epoch [1/5], Step [51/25334], D_loss: 0.5330, G_loss: 15.7260\n",
      "Epoch [1/5], Step [52/25334], D_loss: 0.6012, G_loss: 9.0613\n",
      "Epoch [1/5], Step [53/25334], D_loss: 0.4212, G_loss: 9.4760\n",
      "Epoch [1/5], Step [54/25334], D_loss: 0.5577, G_loss: 20.9459\n",
      "Epoch [1/5], Step [55/25334], D_loss: 0.5039, G_loss: 6.3010\n",
      "Epoch [1/5], Step [56/25334], D_loss: 0.5107, G_loss: 19.0986\n",
      "Epoch [1/5], Step [57/25334], D_loss: 0.4836, G_loss: 12.3768\n",
      "Epoch [1/5], Step [58/25334], D_loss: 0.5946, G_loss: 4.7901\n",
      "Epoch [1/5], Step [59/25334], D_loss: 0.4947, G_loss: 8.8727\n",
      "Epoch [1/5], Step [60/25334], D_loss: 0.7840, G_loss: 3.8933\n",
      "Epoch [1/5], Step [61/25334], D_loss: 0.5102, G_loss: 12.7734\n",
      "Epoch [1/5], Step [62/25334], D_loss: 0.5350, G_loss: 19.2113\n",
      "Epoch [1/5], Step [63/25334], D_loss: 0.7430, G_loss: 4.3676\n",
      "Epoch [1/5], Step [64/25334], D_loss: 0.3938, G_loss: 9.6935\n",
      "Epoch [1/5], Step [65/25334], D_loss: 0.7404, G_loss: 7.0694\n",
      "Epoch [1/5], Step [66/25334], D_loss: 0.4259, G_loss: 22.5781\n",
      "Epoch [1/5], Step [67/25334], D_loss: 0.5063, G_loss: 10.9610\n",
      "Epoch [1/5], Step [68/25334], D_loss: 0.2696, G_loss: 16.1320\n",
      "Epoch [1/5], Step [69/25334], D_loss: 0.3436, G_loss: 18.1555\n",
      "Epoch [1/5], Step [70/25334], D_loss: 0.2990, G_loss: 9.0968\n",
      "Epoch [1/5], Step [71/25334], D_loss: 0.6499, G_loss: 7.2411\n",
      "Epoch [1/5], Step [72/25334], D_loss: 0.3523, G_loss: 27.4867\n",
      "Epoch [1/5], Step [73/25334], D_loss: 0.4972, G_loss: 11.2594\n",
      "Epoch [1/5], Step [74/25334], D_loss: 0.3789, G_loss: 7.0104\n",
      "Epoch [1/5], Step [75/25334], D_loss: 0.3467, G_loss: 14.2669\n",
      "Epoch [1/5], Step [76/25334], D_loss: 0.8444, G_loss: 4.3183\n",
      "Epoch [1/5], Step [77/25334], D_loss: 0.3194, G_loss: 14.4903\n",
      "Epoch [1/5], Step [78/25334], D_loss: 0.2812, G_loss: 9.3730\n",
      "Epoch [1/5], Step [79/25334], D_loss: 1.0409, G_loss: 6.8115\n",
      "Epoch [1/5], Step [80/25334], D_loss: 0.4907, G_loss: 9.4380\n",
      "Epoch [1/5], Step [81/25334], D_loss: 0.2985, G_loss: 9.7507\n",
      "Epoch [1/5], Step [82/25334], D_loss: 0.5095, G_loss: 7.1420\n",
      "Epoch [1/5], Step [83/25334], D_loss: 0.9013, G_loss: 4.1744\n",
      "Epoch [1/5], Step [84/25334], D_loss: 0.3518, G_loss: 12.4135\n",
      "Epoch [1/5], Step [85/25334], D_loss: 0.3421, G_loss: 10.4480\n",
      "Epoch [1/5], Step [86/25334], D_loss: 0.2493, G_loss: 11.4610\n",
      "Epoch [1/5], Step [87/25334], D_loss: 0.5807, G_loss: 6.4569\n",
      "Epoch [1/5], Step [88/25334], D_loss: 0.2666, G_loss: 10.8033\n",
      "Epoch [1/5], Step [89/25334], D_loss: 0.4575, G_loss: 33.1092\n",
      "Epoch [1/5], Step [90/25334], D_loss: 0.1678, G_loss: 12.8252\n",
      "Epoch [1/5], Step [91/25334], D_loss: 0.5417, G_loss: 5.4836\n",
      "Epoch [1/5], Step [92/25334], D_loss: 0.4550, G_loss: 8.8585\n",
      "Epoch [1/5], Step [93/25334], D_loss: 0.2528, G_loss: 8.2521\n",
      "Epoch [1/5], Step [94/25334], D_loss: 0.3631, G_loss: 35.0583\n",
      "Epoch [1/5], Step [95/25334], D_loss: 0.2576, G_loss: 11.2818\n",
      "Epoch [1/5], Step [96/25334], D_loss: 0.2119, G_loss: 13.4516\n",
      "Epoch [1/5], Step [97/25334], D_loss: 1.1627, G_loss: 4.4303\n",
      "Epoch [1/5], Step [98/25334], D_loss: 1.1087, G_loss: 6.4762\n",
      "Epoch [1/5], Step [99/25334], D_loss: 0.7092, G_loss: 5.1181\n",
      "Epoch [1/5], Step [100/25334], D_loss: 0.5661, G_loss: 5.9154\n",
      "Epoch [1/5], Step [101/25334], D_loss: 0.4020, G_loss: 11.0860\n",
      "Epoch [1/5], Step [102/25334], D_loss: 0.3155, G_loss: 24.3250\n",
      "Epoch [1/5], Step [103/25334], D_loss: 0.1822, G_loss: 18.2733\n",
      "Epoch [1/5], Step [104/25334], D_loss: 0.6792, G_loss: 5.9756\n",
      "Epoch [1/5], Step [105/25334], D_loss: 0.1877, G_loss: 12.9326\n",
      "Epoch [1/5], Step [106/25334], D_loss: 0.3163, G_loss: 8.9369\n",
      "Epoch [1/5], Step [107/25334], D_loss: 0.1957, G_loss: 21.9692\n",
      "Epoch [1/5], Step [108/25334], D_loss: 0.4008, G_loss: 6.4141\n",
      "Epoch [1/5], Step [109/25334], D_loss: 0.5557, G_loss: 6.6792\n",
      "Epoch [1/5], Step [110/25334], D_loss: 0.2887, G_loss: 8.2187\n",
      "Epoch [1/5], Step [111/25334], D_loss: 0.1419, G_loss: 15.5212\n",
      "Epoch [1/5], Step [112/25334], D_loss: 0.3682, G_loss: 6.9882\n",
      "Epoch [1/5], Step [113/25334], D_loss: 0.9375, G_loss: 4.2318\n",
      "Epoch [1/5], Step [114/25334], D_loss: 0.7839, G_loss: 6.6333\n",
      "Epoch [1/5], Step [115/25334], D_loss: 0.1883, G_loss: 11.3611\n",
      "Epoch [1/5], Step [116/25334], D_loss: 0.1327, G_loss: 17.6896\n",
      "Epoch [1/5], Step [117/25334], D_loss: 0.2331, G_loss: 20.8315\n",
      "Epoch [1/5], Step [118/25334], D_loss: 0.1981, G_loss: 16.0627\n",
      "Epoch [1/5], Step [119/25334], D_loss: 0.2015, G_loss: 12.5324\n",
      "Epoch [1/5], Step [120/25334], D_loss: 0.2249, G_loss: 31.7051\n",
      "Epoch [1/5], Step [121/25334], D_loss: 0.1921, G_loss: 11.0720\n",
      "Epoch [1/5], Step [122/25334], D_loss: 1.0088, G_loss: 4.3182\n",
      "Epoch [1/5], Step [123/25334], D_loss: 0.9883, G_loss: 3.6070\n",
      "Epoch [1/5], Step [124/25334], D_loss: 0.8731, G_loss: 4.7813\n",
      "Epoch [1/5], Step [125/25334], D_loss: 0.1867, G_loss: 22.8555\n",
      "Epoch [1/5], Step [126/25334], D_loss: 0.1875, G_loss: 15.1591\n",
      "Epoch [1/5], Step [127/25334], D_loss: 0.2603, G_loss: 8.8174\n",
      "Epoch [1/5], Step [128/25334], D_loss: 0.7267, G_loss: 3.8572\n",
      "Epoch [1/5], Step [129/25334], D_loss: 0.6211, G_loss: 12.6481\n",
      "Epoch [1/5], Step [130/25334], D_loss: 0.2943, G_loss: 8.3770\n",
      "Epoch [1/5], Step [131/25334], D_loss: 0.8310, G_loss: 6.8609\n",
      "Epoch [1/5], Step [132/25334], D_loss: 0.1715, G_loss: 16.6441\n",
      "Epoch [1/5], Step [133/25334], D_loss: 0.9407, G_loss: 5.3945\n",
      "Epoch [1/5], Step [134/25334], D_loss: 0.1710, G_loss: 17.8754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [135/25334], D_loss: 0.5983, G_loss: 8.2945\n",
      "Epoch [1/5], Step [136/25334], D_loss: 0.1439, G_loss: 19.2705\n",
      "Epoch [1/5], Step [137/25334], D_loss: 0.3317, G_loss: 7.7724\n",
      "Epoch [1/5], Step [138/25334], D_loss: 0.1441, G_loss: 10.5264\n",
      "Epoch [1/5], Step [139/25334], D_loss: 1.1265, G_loss: 6.4215\n",
      "Epoch [1/5], Step [140/25334], D_loss: 0.2507, G_loss: 21.9660\n",
      "Epoch [1/5], Step [141/25334], D_loss: 0.2390, G_loss: 27.1330\n",
      "Epoch [1/5], Step [142/25334], D_loss: 0.5343, G_loss: 4.7910\n",
      "Epoch [1/5], Step [143/25334], D_loss: 0.2574, G_loss: 6.5666\n",
      "Epoch [1/5], Step [144/25334], D_loss: 0.2068, G_loss: 8.2148\n",
      "Epoch [1/5], Step [145/25334], D_loss: 0.1820, G_loss: 9.5096\n",
      "Epoch [1/5], Step [146/25334], D_loss: 0.9118, G_loss: 7.1165\n",
      "Epoch [1/5], Step [147/25334], D_loss: 0.1542, G_loss: 12.9767\n",
      "Epoch [1/5], Step [148/25334], D_loss: 0.2956, G_loss: 5.9475\n",
      "Epoch [1/5], Step [149/25334], D_loss: 0.3866, G_loss: 8.0579\n",
      "Epoch [1/5], Step [150/25334], D_loss: 0.0954, G_loss: 16.5221\n",
      "Epoch [1/5], Step [151/25334], D_loss: 0.2165, G_loss: 25.8649\n",
      "Epoch [1/5], Step [152/25334], D_loss: 0.1151, G_loss: 10.3958\n",
      "Epoch [1/5], Step [153/25334], D_loss: 0.0772, G_loss: 12.4353\n",
      "Epoch [1/5], Step [154/25334], D_loss: 0.0938, G_loss: 15.7499\n",
      "Epoch [1/5], Step [155/25334], D_loss: 0.6814, G_loss: 7.0024\n",
      "Epoch [1/5], Step [156/25334], D_loss: 0.3814, G_loss: 56.8032\n",
      "Epoch [1/5], Step [157/25334], D_loss: 0.2342, G_loss: 22.9043\n",
      "Epoch [1/5], Step [158/25334], D_loss: 0.5180, G_loss: 7.8958\n",
      "Epoch [1/5], Step [159/25334], D_loss: 0.0942, G_loss: 12.7556\n",
      "Epoch [1/5], Step [160/25334], D_loss: 0.3617, G_loss: 5.7268\n",
      "Epoch [1/5], Step [161/25334], D_loss: 0.1344, G_loss: 14.1969\n",
      "Epoch [1/5], Step [162/25334], D_loss: 0.1129, G_loss: 20.9837\n",
      "Epoch [1/5], Step [163/25334], D_loss: 0.0974, G_loss: 13.5788\n",
      "Epoch [1/5], Step [164/25334], D_loss: 0.2908, G_loss: 8.6990\n",
      "Epoch [1/5], Step [165/25334], D_loss: 0.0920, G_loss: 10.0667\n",
      "Epoch [1/5], Step [166/25334], D_loss: 0.8013, G_loss: 4.1469\n",
      "Epoch [1/5], Step [167/25334], D_loss: 0.1184, G_loss: 9.1210\n",
      "Epoch [1/5], Step [168/25334], D_loss: 0.1627, G_loss: 16.1559\n",
      "Epoch [1/5], Step [169/25334], D_loss: 1.6174, G_loss: 7.1566\n",
      "Epoch [1/5], Step [170/25334], D_loss: 0.1255, G_loss: 19.6426\n",
      "Epoch [1/5], Step [171/25334], D_loss: 0.5479, G_loss: 6.6509\n",
      "Epoch [1/5], Step [172/25334], D_loss: 0.1952, G_loss: 10.0746\n",
      "Epoch [1/5], Step [173/25334], D_loss: 0.9247, G_loss: 3.2297\n",
      "Epoch [1/5], Step [174/25334], D_loss: 0.1690, G_loss: 9.3031\n",
      "Epoch [1/5], Step [175/25334], D_loss: 0.1929, G_loss: 10.6040\n",
      "Epoch [1/5], Step [176/25334], D_loss: 0.8025, G_loss: 3.9192\n",
      "Epoch [1/5], Step [177/25334], D_loss: 0.0985, G_loss: 12.3505\n",
      "Epoch [1/5], Step [178/25334], D_loss: 0.1006, G_loss: 21.2800\n",
      "Epoch [1/5], Step [179/25334], D_loss: 0.6827, G_loss: 5.5933\n",
      "Epoch [1/5], Step [180/25334], D_loss: 0.0969, G_loss: 13.1360\n",
      "Epoch [1/5], Step [181/25334], D_loss: 0.0963, G_loss: 12.3807\n",
      "Epoch [1/5], Step [182/25334], D_loss: 0.0912, G_loss: 15.2635\n",
      "Epoch [1/5], Step [183/25334], D_loss: 0.0605, G_loss: 12.8357\n",
      "Epoch [1/5], Step [184/25334], D_loss: 0.0619, G_loss: 20.9681\n",
      "Epoch [1/5], Step [185/25334], D_loss: 0.2083, G_loss: 44.2405\n",
      "Epoch [1/5], Step [186/25334], D_loss: 0.7257, G_loss: 2.9303\n",
      "Epoch [1/5], Step [187/25334], D_loss: 0.1279, G_loss: 9.9776\n",
      "Epoch [1/5], Step [188/25334], D_loss: 0.1000, G_loss: 11.4209\n",
      "Epoch [1/5], Step [189/25334], D_loss: 0.4733, G_loss: 6.8698\n",
      "Epoch [1/5], Step [190/25334], D_loss: 0.1041, G_loss: 31.4898\n",
      "Epoch [1/5], Step [191/25334], D_loss: 0.0810, G_loss: 11.1630\n",
      "Epoch [1/5], Step [192/25334], D_loss: 0.2403, G_loss: 51.7705\n",
      "Epoch [1/5], Step [193/25334], D_loss: 0.0512, G_loss: 13.8746\n",
      "Epoch [1/5], Step [194/25334], D_loss: 0.7662, G_loss: 4.9478\n",
      "Epoch [1/5], Step [195/25334], D_loss: 0.9487, G_loss: 6.8547\n",
      "Epoch [1/5], Step [196/25334], D_loss: 0.0802, G_loss: 15.1232\n",
      "Epoch [1/5], Step [197/25334], D_loss: 0.6195, G_loss: 4.1521\n",
      "Epoch [1/5], Step [198/25334], D_loss: 0.0986, G_loss: 11.3473\n",
      "Epoch [1/5], Step [199/25334], D_loss: 0.0944, G_loss: 11.5451\n",
      "Epoch [1/5], Step [200/25334], D_loss: 0.0866, G_loss: 10.2209\n",
      "Epoch [1/5], Step [201/25334], D_loss: 0.8375, G_loss: 7.9566\n",
      "Epoch [1/5], Step [202/25334], D_loss: 0.0872, G_loss: 12.5883\n",
      "Epoch [1/5], Step [203/25334], D_loss: 0.0758, G_loss: 10.0121\n",
      "Epoch [1/5], Step [204/25334], D_loss: 0.0711, G_loss: 17.2746\n",
      "Epoch [1/5], Step [205/25334], D_loss: 0.7342, G_loss: 3.1426\n",
      "Epoch [1/5], Step [206/25334], D_loss: 0.3340, G_loss: 6.4653\n",
      "Epoch [1/5], Step [207/25334], D_loss: 0.6580, G_loss: 6.0782\n",
      "Epoch [1/5], Step [208/25334], D_loss: 0.2249, G_loss: 6.1361\n",
      "Epoch [1/5], Step [209/25334], D_loss: 0.7255, G_loss: 4.3988\n",
      "Epoch [1/5], Step [210/25334], D_loss: 0.6043, G_loss: 6.4227\n",
      "Epoch [1/5], Step [211/25334], D_loss: 0.0605, G_loss: 16.1680\n",
      "Epoch [1/5], Step [212/25334], D_loss: 0.0600, G_loss: 15.7887\n",
      "Epoch [1/5], Step [213/25334], D_loss: 0.2178, G_loss: 10.7478\n",
      "Epoch [1/5], Step [214/25334], D_loss: 0.1195, G_loss: 7.0636\n",
      "Epoch [1/5], Step [215/25334], D_loss: 0.0780, G_loss: 11.8817\n",
      "Epoch [1/5], Step [216/25334], D_loss: 0.0900, G_loss: 12.1990\n",
      "Epoch [1/5], Step [217/25334], D_loss: 0.0413, G_loss: 15.5205\n",
      "Epoch [1/5], Step [218/25334], D_loss: 0.0377, G_loss: 20.0331\n",
      "Epoch [1/5], Step [219/25334], D_loss: 0.0954, G_loss: 7.4343\n",
      "Epoch [1/5], Step [220/25334], D_loss: 0.0661, G_loss: 12.9739\n",
      "Epoch [1/5], Step [221/25334], D_loss: 0.0348, G_loss: 17.3500\n",
      "Epoch [1/5], Step [222/25334], D_loss: 0.0550, G_loss: 17.1433\n",
      "Epoch [1/5], Step [223/25334], D_loss: 0.1204, G_loss: 10.3462\n",
      "Epoch [1/5], Step [224/25334], D_loss: 0.0528, G_loss: 26.3966\n",
      "Epoch [1/5], Step [225/25334], D_loss: 0.0446, G_loss: 11.9322\n",
      "Epoch [1/5], Step [226/25334], D_loss: 0.0312, G_loss: 13.5268\n",
      "Epoch [1/5], Step [227/25334], D_loss: 0.0489, G_loss: 12.2647\n",
      "Epoch [1/5], Step [228/25334], D_loss: 0.0382, G_loss: 13.5312\n",
      "Epoch [1/5], Step [229/25334], D_loss: 0.0476, G_loss: 20.1269\n",
      "Epoch [1/5], Step [230/25334], D_loss: 0.8582, G_loss: 4.6093\n",
      "Epoch [1/5], Step [231/25334], D_loss: 0.0697, G_loss: 9.1343\n",
      "Epoch [1/5], Step [232/25334], D_loss: 0.0582, G_loss: 10.6822\n",
      "Epoch [1/5], Step [233/25334], D_loss: 0.0359, G_loss: 24.6820\n",
      "Epoch [1/5], Step [234/25334], D_loss: 0.0381, G_loss: 23.1741\n",
      "Epoch [1/5], Step [235/25334], D_loss: 0.6790, G_loss: 3.8809\n",
      "Epoch [1/5], Step [236/25334], D_loss: 0.0306, G_loss: 17.2670\n",
      "Epoch [1/5], Step [237/25334], D_loss: 0.0293, G_loss: 14.3544\n",
      "Epoch [1/5], Step [238/25334], D_loss: 0.0255, G_loss: 15.1486\n",
      "Epoch [1/5], Step [239/25334], D_loss: 0.0362, G_loss: 14.5389\n",
      "Epoch [1/5], Step [240/25334], D_loss: 0.0343, G_loss: 9.8066\n",
      "Epoch [1/5], Step [241/25334], D_loss: 0.0664, G_loss: 15.9876\n",
      "Epoch [1/5], Step [242/25334], D_loss: 0.0330, G_loss: 13.1444\n",
      "Epoch [1/5], Step [243/25334], D_loss: 0.0615, G_loss: 16.0931\n",
      "Epoch [1/5], Step [244/25334], D_loss: 0.0511, G_loss: 13.3845\n",
      "Epoch [1/5], Step [245/25334], D_loss: 0.3167, G_loss: 5.4753\n",
      "Epoch [1/5], Step [246/25334], D_loss: 0.0299, G_loss: 16.6374\n",
      "Epoch [1/5], Step [247/25334], D_loss: 0.8336, G_loss: 5.2568\n",
      "Epoch [1/5], Step [248/25334], D_loss: 0.0368, G_loss: 14.8490\n",
      "Epoch [1/5], Step [249/25334], D_loss: 1.2692, G_loss: 6.1854\n",
      "Epoch [1/5], Step [250/25334], D_loss: 0.1246, G_loss: 12.7896\n",
      "Epoch [1/5], Step [251/25334], D_loss: 0.1555, G_loss: 12.9205\n",
      "Epoch [1/5], Step [252/25334], D_loss: 0.4278, G_loss: 8.7716\n",
      "Epoch [1/5], Step [253/25334], D_loss: 0.2966, G_loss: 8.0939\n",
      "Epoch [1/5], Step [254/25334], D_loss: 0.8582, G_loss: 3.1300\n",
      "Epoch [1/5], Step [255/25334], D_loss: 0.2866, G_loss: 14.8771\n",
      "Epoch [1/5], Step [256/25334], D_loss: 0.1255, G_loss: 18.4117\n",
      "Epoch [1/5], Step [257/25334], D_loss: 1.1434, G_loss: 10.1927\n",
      "Epoch [1/5], Step [258/25334], D_loss: 0.6115, G_loss: 3.9337\n",
      "Epoch [1/5], Step [259/25334], D_loss: 0.1009, G_loss: 16.7664\n",
      "Epoch [1/5], Step [260/25334], D_loss: 0.2025, G_loss: 10.1165\n",
      "Epoch [1/5], Step [261/25334], D_loss: 0.0414, G_loss: 14.5676\n",
      "Epoch [1/5], Step [262/25334], D_loss: 0.3037, G_loss: 6.2196\n",
      "Epoch [1/5], Step [263/25334], D_loss: 0.7868, G_loss: 4.5657\n",
      "Epoch [1/5], Step [264/25334], D_loss: 0.1371, G_loss: 11.6397\n",
      "Epoch [1/5], Step [265/25334], D_loss: 0.0656, G_loss: 9.8213\n",
      "Epoch [1/5], Step [266/25334], D_loss: 0.1034, G_loss: 8.7912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [267/25334], D_loss: 0.0317, G_loss: 17.6813\n",
      "Epoch [1/5], Step [268/25334], D_loss: 0.1173, G_loss: 7.3410\n",
      "Epoch [1/5], Step [269/25334], D_loss: 1.0579, G_loss: 5.9651\n",
      "Epoch [1/5], Step [270/25334], D_loss: 0.2694, G_loss: 6.0780\n",
      "Epoch [1/5], Step [271/25334], D_loss: 0.0971, G_loss: 9.3624\n",
      "Epoch [1/5], Step [272/25334], D_loss: 0.5851, G_loss: 4.0565\n",
      "Epoch [1/5], Step [273/25334], D_loss: 0.0525, G_loss: 11.7829\n",
      "Epoch [1/5], Step [274/25334], D_loss: 0.0511, G_loss: 12.9159\n",
      "Epoch [1/5], Step [275/25334], D_loss: 0.0585, G_loss: 13.2522\n",
      "Epoch [1/5], Step [276/25334], D_loss: 0.0789, G_loss: 15.5879\n",
      "Epoch [1/5], Step [277/25334], D_loss: 0.0408, G_loss: 15.4976\n",
      "Epoch [1/5], Step [278/25334], D_loss: 0.0458, G_loss: 10.8296\n",
      "Epoch [1/5], Step [279/25334], D_loss: 0.1384, G_loss: 34.6628\n",
      "Epoch [1/5], Step [280/25334], D_loss: 0.2667, G_loss: 6.0647\n",
      "Epoch [1/5], Step [281/25334], D_loss: 0.0196, G_loss: 16.4618\n",
      "Epoch [1/5], Step [282/25334], D_loss: 0.1302, G_loss: 14.6368\n",
      "Epoch [1/5], Step [283/25334], D_loss: 0.6737, G_loss: 3.1923\n",
      "Epoch [1/5], Step [284/25334], D_loss: 0.0371, G_loss: 19.8856\n",
      "Epoch [1/5], Step [285/25334], D_loss: 0.0534, G_loss: 12.0911\n",
      "Epoch [1/5], Step [286/25334], D_loss: 0.4855, G_loss: 6.0344\n",
      "Epoch [1/5], Step [287/25334], D_loss: 0.0357, G_loss: 23.4655\n",
      "Epoch [1/5], Step [288/25334], D_loss: 0.1147, G_loss: 7.5898\n",
      "Epoch [1/5], Step [289/25334], D_loss: 0.8492, G_loss: 4.3569\n",
      "Epoch [1/5], Step [290/25334], D_loss: 0.0593, G_loss: 21.9988\n",
      "Epoch [1/5], Step [291/25334], D_loss: 0.0489, G_loss: 23.1188\n",
      "Epoch [1/5], Step [292/25334], D_loss: 0.0547, G_loss: 14.7587\n",
      "Epoch [1/5], Step [293/25334], D_loss: 0.0306, G_loss: 18.3262\n",
      "Epoch [1/5], Step [294/25334], D_loss: 0.2068, G_loss: 5.7406\n",
      "Epoch [1/5], Step [295/25334], D_loss: 0.0627, G_loss: 8.6716\n",
      "Epoch [1/5], Step [296/25334], D_loss: 0.6358, G_loss: 3.1748\n",
      "Epoch [1/5], Step [297/25334], D_loss: 0.0751, G_loss: 7.1966\n",
      "Epoch [1/5], Step [298/25334], D_loss: 0.1024, G_loss: 7.9671\n",
      "Epoch [1/5], Step [299/25334], D_loss: 0.7256, G_loss: 4.2208\n",
      "Epoch [1/5], Step [300/25334], D_loss: 0.4204, G_loss: 4.6771\n",
      "Epoch [1/5], Step [301/25334], D_loss: 0.5056, G_loss: 5.3393\n",
      "Epoch [1/5], Step [302/25334], D_loss: 0.0586, G_loss: 26.6427\n",
      "Epoch [1/5], Step [303/25334], D_loss: 0.0178, G_loss: 15.6787\n",
      "Epoch [1/5], Step [304/25334], D_loss: 0.0825, G_loss: 11.0284\n",
      "Epoch [1/5], Step [305/25334], D_loss: 0.0352, G_loss: 14.6727\n",
      "Epoch [1/5], Step [306/25334], D_loss: 0.6577, G_loss: 4.9359\n",
      "Epoch [1/5], Step [307/25334], D_loss: 0.0249, G_loss: 11.6886\n",
      "Epoch [1/5], Step [308/25334], D_loss: 0.7707, G_loss: 5.7530\n",
      "Epoch [1/5], Step [309/25334], D_loss: 0.0229, G_loss: 26.9970\n",
      "Epoch [1/5], Step [310/25334], D_loss: 0.0408, G_loss: 23.9673\n",
      "Epoch [1/5], Step [311/25334], D_loss: 0.0389, G_loss: 16.1114\n",
      "Epoch [1/5], Step [312/25334], D_loss: 0.0287, G_loss: 12.1399\n",
      "Epoch [1/5], Step [313/25334], D_loss: 0.5129, G_loss: 6.2307\n",
      "Epoch [1/5], Step [314/25334], D_loss: 0.0272, G_loss: 13.8939\n",
      "Epoch [1/5], Step [315/25334], D_loss: 0.0522, G_loss: 18.1579\n",
      "Epoch [1/5], Step [316/25334], D_loss: 0.0352, G_loss: 15.2063\n",
      "Epoch [1/5], Step [317/25334], D_loss: 0.2907, G_loss: 11.0808\n",
      "Epoch [1/5], Step [318/25334], D_loss: 0.2680, G_loss: 10.0897\n",
      "Epoch [1/5], Step [319/25334], D_loss: 0.2057, G_loss: 7.3722\n",
      "Epoch [1/5], Step [320/25334], D_loss: 0.0346, G_loss: 11.9702\n",
      "Epoch [1/5], Step [321/25334], D_loss: 0.0367, G_loss: 20.6806\n",
      "Epoch [1/5], Step [322/25334], D_loss: 0.0439, G_loss: 10.4540\n",
      "Epoch [1/5], Step [323/25334], D_loss: 0.0227, G_loss: 12.2919\n",
      "Epoch [1/5], Step [324/25334], D_loss: 0.0231, G_loss: 11.6815\n",
      "Epoch [1/5], Step [325/25334], D_loss: 0.0474, G_loss: 8.2485\n",
      "Epoch [1/5], Step [326/25334], D_loss: 1.5223, G_loss: 6.8462\n",
      "Epoch [1/5], Step [327/25334], D_loss: 0.4323, G_loss: 4.0435\n",
      "Epoch [1/5], Step [328/25334], D_loss: 0.1717, G_loss: 10.4064\n",
      "Epoch [1/5], Step [329/25334], D_loss: 0.0752, G_loss: 18.5586\n",
      "Epoch [1/5], Step [330/25334], D_loss: 0.0601, G_loss: 10.4806\n",
      "Epoch [1/5], Step [331/25334], D_loss: 0.1406, G_loss: 6.3098\n",
      "Epoch [1/5], Step [332/25334], D_loss: 0.0882, G_loss: 42.2165\n",
      "Epoch [1/5], Step [333/25334], D_loss: 0.4243, G_loss: 5.6402\n",
      "Epoch [1/5], Step [334/25334], D_loss: 0.0415, G_loss: 10.4655\n",
      "Epoch [1/5], Step [335/25334], D_loss: 0.0417, G_loss: 12.4699\n",
      "Epoch [1/5], Step [336/25334], D_loss: 0.4397, G_loss: 8.7506\n",
      "Epoch [1/5], Step [337/25334], D_loss: 0.9196, G_loss: 6.4913\n",
      "Epoch [1/5], Step [338/25334], D_loss: 0.0336, G_loss: 17.5337\n",
      "Epoch [1/5], Step [339/25334], D_loss: 0.0709, G_loss: 11.7503\n",
      "Epoch [1/5], Step [340/25334], D_loss: 0.1349, G_loss: 8.6880\n",
      "Epoch [1/5], Step [341/25334], D_loss: 0.0507, G_loss: 12.2461\n",
      "Epoch [1/5], Step [342/25334], D_loss: 0.0580, G_loss: 19.1734\n",
      "Epoch [1/5], Step [343/25334], D_loss: 0.0306, G_loss: 18.5340\n",
      "Epoch [1/5], Step [344/25334], D_loss: 0.6960, G_loss: 4.8022\n",
      "Epoch [1/5], Step [345/25334], D_loss: 0.0221, G_loss: 19.2405\n",
      "Epoch [1/5], Step [346/25334], D_loss: 0.8444, G_loss: 6.8040\n",
      "Epoch [1/5], Step [347/25334], D_loss: 0.0521, G_loss: 13.0980\n",
      "Epoch [1/5], Step [348/25334], D_loss: 0.9347, G_loss: 5.4881\n",
      "Epoch [1/5], Step [349/25334], D_loss: 0.0685, G_loss: 8.9345\n",
      "Epoch [1/5], Step [350/25334], D_loss: 0.3076, G_loss: 4.9872\n",
      "Epoch [1/5], Step [351/25334], D_loss: 0.6946, G_loss: 4.5122\n",
      "Epoch [1/5], Step [352/25334], D_loss: 0.0434, G_loss: 11.6538\n",
      "Epoch [1/5], Step [353/25334], D_loss: 0.7630, G_loss: 4.5185\n",
      "Epoch [1/5], Step [354/25334], D_loss: 0.0673, G_loss: 9.4793\n",
      "Epoch [1/5], Step [355/25334], D_loss: 0.7472, G_loss: 3.8219\n",
      "Epoch [1/5], Step [356/25334], D_loss: 0.7153, G_loss: 3.4693\n",
      "Epoch [1/5], Step [357/25334], D_loss: 0.1203, G_loss: 10.3222\n",
      "Epoch [1/5], Step [358/25334], D_loss: 0.0351, G_loss: 14.1249\n",
      "Epoch [1/5], Step [359/25334], D_loss: 0.0198, G_loss: 20.1427\n",
      "Epoch [1/5], Step [360/25334], D_loss: 0.7050, G_loss: 3.2805\n",
      "Epoch [1/5], Step [361/25334], D_loss: 0.6216, G_loss: 5.4426\n",
      "Epoch [1/5], Step [362/25334], D_loss: 0.7628, G_loss: 2.9674\n",
      "Epoch [1/5], Step [363/25334], D_loss: 0.0466, G_loss: 9.7433\n",
      "Epoch [1/5], Step [364/25334], D_loss: 0.0696, G_loss: 8.4615\n",
      "Epoch [1/5], Step [365/25334], D_loss: 0.0211, G_loss: 15.7177\n",
      "Epoch [1/5], Step [366/25334], D_loss: 0.4827, G_loss: 4.5038\n",
      "Epoch [1/5], Step [367/25334], D_loss: 0.2813, G_loss: 6.7973\n",
      "Epoch [1/5], Step [368/25334], D_loss: 0.7494, G_loss: 3.7531\n",
      "Epoch [1/5], Step [369/25334], D_loss: 0.2173, G_loss: 6.2357\n",
      "Epoch [1/5], Step [370/25334], D_loss: 0.0811, G_loss: 7.3692\n",
      "Epoch [1/5], Step [371/25334], D_loss: 0.0739, G_loss: 38.3126\n",
      "Epoch [1/5], Step [372/25334], D_loss: 0.0157, G_loss: 16.5545\n",
      "Epoch [1/5], Step [373/25334], D_loss: 0.3760, G_loss: 8.5389\n",
      "Epoch [1/5], Step [374/25334], D_loss: 0.2938, G_loss: 3.8969\n",
      "Epoch [1/5], Step [375/25334], D_loss: 0.0721, G_loss: 8.6984\n",
      "Epoch [1/5], Step [376/25334], D_loss: 0.0192, G_loss: 10.1892\n",
      "Epoch [1/5], Step [377/25334], D_loss: 0.1213, G_loss: 21.9831\n",
      "Epoch [1/5], Step [378/25334], D_loss: 0.0278, G_loss: 13.6809\n",
      "Epoch [1/5], Step [379/25334], D_loss: 0.0275, G_loss: 15.8241\n",
      "Epoch [1/5], Step [380/25334], D_loss: 0.1250, G_loss: 8.1976\n",
      "Epoch [1/5], Step [381/25334], D_loss: 0.0207, G_loss: 16.0720\n",
      "Epoch [1/5], Step [382/25334], D_loss: 0.0258, G_loss: 15.3482\n",
      "Epoch [1/5], Step [383/25334], D_loss: 0.0675, G_loss: 8.2113\n",
      "Epoch [1/5], Step [384/25334], D_loss: 0.0144, G_loss: 14.6082\n",
      "Epoch [1/5], Step [385/25334], D_loss: 0.0209, G_loss: 19.6011\n",
      "Epoch [1/5], Step [386/25334], D_loss: 0.0154, G_loss: 23.3986\n",
      "Epoch [1/5], Step [387/25334], D_loss: 0.0163, G_loss: 14.4086\n",
      "Epoch [1/5], Step [388/25334], D_loss: 0.0125, G_loss: 18.1708\n",
      "Epoch [1/5], Step [389/25334], D_loss: 0.0558, G_loss: 7.7147\n",
      "Epoch [1/5], Step [390/25334], D_loss: 0.6927, G_loss: 5.8848\n",
      "Epoch [1/5], Step [391/25334], D_loss: 0.0262, G_loss: 12.9923\n",
      "Epoch [1/5], Step [392/25334], D_loss: 0.0509, G_loss: 11.1529\n",
      "Epoch [1/5], Step [393/25334], D_loss: 0.0282, G_loss: 13.8658\n",
      "Epoch [1/5], Step [394/25334], D_loss: 0.2054, G_loss: 6.3600\n",
      "Epoch [1/5], Step [395/25334], D_loss: 0.0273, G_loss: 21.1342\n",
      "Epoch [1/5], Step [396/25334], D_loss: 0.3893, G_loss: 4.8805\n",
      "Epoch [1/5], Step [397/25334], D_loss: 0.1111, G_loss: 11.5245\n",
      "Epoch [1/5], Step [398/25334], D_loss: 0.0163, G_loss: 23.8395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [399/25334], D_loss: 0.0203, G_loss: 14.8956\n",
      "Epoch [1/5], Step [400/25334], D_loss: 1.0954, G_loss: 5.3711\n",
      "Epoch [1/5], Step [401/25334], D_loss: 0.1155, G_loss: 34.3722\n",
      "Epoch [1/5], Step [402/25334], D_loss: 0.6076, G_loss: 3.6001\n",
      "Epoch [1/5], Step [403/25334], D_loss: 0.6027, G_loss: 5.0253\n",
      "Epoch [1/5], Step [404/25334], D_loss: 0.0331, G_loss: 12.4458\n",
      "Epoch [1/5], Step [405/25334], D_loss: 0.0413, G_loss: 13.1672\n",
      "Epoch [1/5], Step [406/25334], D_loss: 0.7856, G_loss: 3.4354\n",
      "Epoch [1/5], Step [407/25334], D_loss: 0.2246, G_loss: 7.5372\n",
      "Epoch [1/5], Step [408/25334], D_loss: 0.0763, G_loss: 12.5500\n",
      "Epoch [1/5], Step [409/25334], D_loss: 0.5760, G_loss: 3.8804\n",
      "Epoch [1/5], Step [410/25334], D_loss: 0.0341, G_loss: 14.8443\n",
      "Epoch [1/5], Step [411/25334], D_loss: 0.0295, G_loss: 17.6016\n",
      "Epoch [1/5], Step [412/25334], D_loss: 0.0541, G_loss: 12.1981\n",
      "Epoch [1/5], Step [413/25334], D_loss: 0.0160, G_loss: 19.0122\n",
      "Epoch [1/5], Step [414/25334], D_loss: 0.0176, G_loss: 23.1028\n",
      "Epoch [1/5], Step [415/25334], D_loss: 0.2348, G_loss: 7.2839\n",
      "Epoch [1/5], Step [416/25334], D_loss: 0.1483, G_loss: 7.1462\n",
      "Epoch [1/5], Step [417/25334], D_loss: 0.4156, G_loss: 4.8730\n",
      "Epoch [1/5], Step [418/25334], D_loss: 0.0337, G_loss: 8.5048\n",
      "Epoch [1/5], Step [419/25334], D_loss: 0.0126, G_loss: 14.5093\n",
      "Epoch [1/5], Step [420/25334], D_loss: 0.0215, G_loss: 19.0253\n",
      "Epoch [1/5], Step [421/25334], D_loss: 0.0271, G_loss: 12.6065\n",
      "Epoch [1/5], Step [422/25334], D_loss: 0.0241, G_loss: 22.5711\n",
      "Epoch [1/5], Step [423/25334], D_loss: 0.0159, G_loss: 14.9173\n",
      "Epoch [1/5], Step [424/25334], D_loss: 0.0182, G_loss: 16.7616\n",
      "Epoch [1/5], Step [425/25334], D_loss: 0.0225, G_loss: 10.3435\n",
      "Epoch [1/5], Step [426/25334], D_loss: 0.0206, G_loss: 14.7471\n",
      "Epoch [1/5], Step [427/25334], D_loss: 0.6068, G_loss: 3.4244\n",
      "Epoch [1/5], Step [428/25334], D_loss: 0.0408, G_loss: 14.8958\n",
      "Epoch [1/5], Step [429/25334], D_loss: 0.0197, G_loss: 10.9947\n",
      "Epoch [1/5], Step [430/25334], D_loss: 0.0212, G_loss: 12.4919\n",
      "Epoch [1/5], Step [431/25334], D_loss: 0.0197, G_loss: 15.3160\n",
      "Epoch [1/5], Step [432/25334], D_loss: 0.2588, G_loss: 8.8941\n",
      "Epoch [1/5], Step [433/25334], D_loss: 0.4858, G_loss: 5.4427\n",
      "Epoch [1/5], Step [434/25334], D_loss: 0.0259, G_loss: 14.8794\n",
      "Epoch [1/5], Step [435/25334], D_loss: 0.1817, G_loss: 5.7589\n",
      "Epoch [1/5], Step [436/25334], D_loss: 0.0242, G_loss: 25.2247\n",
      "Epoch [1/5], Step [437/25334], D_loss: 0.0534, G_loss: 6.6081\n",
      "Epoch [1/5], Step [438/25334], D_loss: 0.0704, G_loss: 11.5140\n",
      "Epoch [1/5], Step [439/25334], D_loss: 0.0162, G_loss: 26.7572\n",
      "Epoch [1/5], Step [440/25334], D_loss: 0.0127, G_loss: 16.9144\n",
      "Epoch [1/5], Step [441/25334], D_loss: 0.0912, G_loss: 8.5912\n",
      "Epoch [1/5], Step [442/25334], D_loss: 0.0297, G_loss: 9.1445\n",
      "Epoch [1/5], Step [443/25334], D_loss: 0.0115, G_loss: 22.5140\n",
      "Epoch [1/5], Step [444/25334], D_loss: 1.0542, G_loss: 10.0318\n",
      "Epoch [1/5], Step [445/25334], D_loss: 0.2256, G_loss: 3.7739\n",
      "Epoch [1/5], Step [446/25334], D_loss: 0.0413, G_loss: 9.6046\n",
      "Epoch [1/5], Step [447/25334], D_loss: 0.8506, G_loss: 4.9860\n",
      "Epoch [1/5], Step [448/25334], D_loss: 0.0463, G_loss: 7.1984\n",
      "Epoch [1/5], Step [449/25334], D_loss: 0.4951, G_loss: 4.9976\n",
      "Epoch [1/5], Step [450/25334], D_loss: 0.2778, G_loss: 15.2206\n",
      "Epoch [1/5], Step [451/25334], D_loss: 0.3613, G_loss: 9.5532\n",
      "Epoch [1/5], Step [452/25334], D_loss: 0.4683, G_loss: 8.2841\n",
      "Epoch [1/5], Step [453/25334], D_loss: 0.3398, G_loss: 5.8844\n",
      "Epoch [1/5], Step [454/25334], D_loss: 0.2082, G_loss: 9.7115\n",
      "Epoch [1/5], Step [455/25334], D_loss: 0.4585, G_loss: 4.5011\n",
      "Epoch [1/5], Step [456/25334], D_loss: 0.0466, G_loss: 12.6791\n",
      "Epoch [1/5], Step [457/25334], D_loss: 0.0272, G_loss: 11.7668\n",
      "Epoch [1/5], Step [458/25334], D_loss: 0.0422, G_loss: 16.7523\n",
      "Epoch [1/5], Step [459/25334], D_loss: 0.6402, G_loss: 5.1173\n",
      "Epoch [1/5], Step [460/25334], D_loss: 0.0598, G_loss: 10.4379\n",
      "Epoch [1/5], Step [461/25334], D_loss: 0.0496, G_loss: 20.9509\n",
      "Epoch [1/5], Step [462/25334], D_loss: 0.0436, G_loss: 9.6932\n",
      "Epoch [1/5], Step [463/25334], D_loss: 0.2387, G_loss: 5.6905\n",
      "Epoch [1/5], Step [464/25334], D_loss: 0.0382, G_loss: 14.1965\n",
      "Epoch [1/5], Step [465/25334], D_loss: 0.0525, G_loss: 29.9035\n",
      "Epoch [1/5], Step [466/25334], D_loss: 0.0657, G_loss: 6.8102\n",
      "Epoch [1/5], Step [467/25334], D_loss: 0.0955, G_loss: 8.9527\n",
      "Epoch [1/5], Step [468/25334], D_loss: 0.0879, G_loss: 6.9971\n",
      "Epoch [1/5], Step [469/25334], D_loss: 0.0210, G_loss: 14.8357\n",
      "Epoch [1/5], Step [470/25334], D_loss: 0.0223, G_loss: 10.6631\n",
      "Epoch [1/5], Step [471/25334], D_loss: 0.0241, G_loss: 7.5931\n",
      "Epoch [1/5], Step [472/25334], D_loss: 0.3369, G_loss: 5.8799\n",
      "Epoch [1/5], Step [473/25334], D_loss: 0.0121, G_loss: 18.3365\n",
      "Epoch [1/5], Step [474/25334], D_loss: 0.0110, G_loss: 14.0199\n",
      "Epoch [1/5], Step [475/25334], D_loss: 0.7015, G_loss: 11.6027\n",
      "Epoch [1/5], Step [476/25334], D_loss: 0.0230, G_loss: 15.5752\n",
      "Epoch [1/5], Step [477/25334], D_loss: 0.0762, G_loss: 7.1790\n",
      "Epoch [1/5], Step [478/25334], D_loss: 0.1416, G_loss: 11.9919\n",
      "Epoch [1/5], Step [479/25334], D_loss: 0.0489, G_loss: 9.9179\n",
      "Epoch [1/5], Step [480/25334], D_loss: 0.1371, G_loss: 7.1342\n",
      "Epoch [1/5], Step [481/25334], D_loss: 0.0296, G_loss: 12.3277\n",
      "Epoch [1/5], Step [482/25334], D_loss: 0.0166, G_loss: 10.9733\n",
      "Epoch [1/5], Step [483/25334], D_loss: 0.0512, G_loss: 8.4801\n",
      "Epoch [1/5], Step [484/25334], D_loss: 0.1016, G_loss: 6.1273\n",
      "Epoch [1/5], Step [485/25334], D_loss: 0.0236, G_loss: 11.1488\n",
      "Epoch [1/5], Step [486/25334], D_loss: 0.0124, G_loss: 12.7073\n",
      "Epoch [1/5], Step [487/25334], D_loss: 0.0191, G_loss: 11.9035\n",
      "Epoch [1/5], Step [488/25334], D_loss: 0.4872, G_loss: 4.2088\n",
      "Epoch [1/5], Step [489/25334], D_loss: 0.0165, G_loss: 17.3518\n",
      "Epoch [1/5], Step [490/25334], D_loss: 0.0811, G_loss: 11.0080\n",
      "Epoch [1/5], Step [491/25334], D_loss: 0.0216, G_loss: 12.7606\n",
      "Epoch [1/5], Step [492/25334], D_loss: 0.0405, G_loss: 9.6234\n",
      "Epoch [1/5], Step [493/25334], D_loss: 0.0236, G_loss: 9.8019\n",
      "Epoch [1/5], Step [494/25334], D_loss: 0.1079, G_loss: 7.4806\n",
      "Epoch [1/5], Step [495/25334], D_loss: 0.0167, G_loss: 17.9715\n",
      "Epoch [1/5], Step [496/25334], D_loss: 0.0253, G_loss: 22.2238\n",
      "Epoch [1/5], Step [497/25334], D_loss: 0.0390, G_loss: 10.1032\n",
      "Epoch [1/5], Step [498/25334], D_loss: 0.0226, G_loss: 11.2112\n",
      "Epoch [1/5], Step [499/25334], D_loss: 0.0464, G_loss: 11.3276\n",
      "Epoch [1/5], Step [500/25334], D_loss: 0.0291, G_loss: 10.4522\n",
      "Epoch [1/5], Step [501/25334], D_loss: 0.7488, G_loss: 10.3810\n",
      "Epoch [1/5], Step [502/25334], D_loss: 0.0409, G_loss: 33.1130\n",
      "Epoch [1/5], Step [503/25334], D_loss: 0.0628, G_loss: 9.1587\n",
      "Epoch [1/5], Step [504/25334], D_loss: 0.1536, G_loss: 6.4968\n",
      "Epoch [1/5], Step [505/25334], D_loss: 0.1565, G_loss: 7.3517\n",
      "Epoch [1/5], Step [506/25334], D_loss: 0.0272, G_loss: 10.9722\n",
      "Epoch [1/5], Step [507/25334], D_loss: 0.0408, G_loss: 33.6634\n",
      "Epoch [1/5], Step [508/25334], D_loss: 0.0300, G_loss: 17.1975\n",
      "Epoch [1/5], Step [509/25334], D_loss: 0.0231, G_loss: 13.2284\n",
      "Epoch [1/5], Step [510/25334], D_loss: 0.1268, G_loss: 6.4148\n",
      "Epoch [1/5], Step [511/25334], D_loss: 0.0233, G_loss: 20.3724\n",
      "Epoch [1/5], Step [512/25334], D_loss: 0.3351, G_loss: 4.9221\n",
      "Epoch [1/5], Step [513/25334], D_loss: 0.1321, G_loss: 8.1032\n",
      "Epoch [1/5], Step [514/25334], D_loss: 0.0140, G_loss: 11.4207\n",
      "Epoch [1/5], Step [515/25334], D_loss: 0.1688, G_loss: 9.2016\n",
      "Epoch [1/5], Step [516/25334], D_loss: 0.0812, G_loss: 7.2287\n",
      "Epoch [1/5], Step [517/25334], D_loss: 0.0270, G_loss: 12.8623\n",
      "Epoch [1/5], Step [518/25334], D_loss: 0.1397, G_loss: 9.8517\n",
      "Epoch [1/5], Step [519/25334], D_loss: 0.0359, G_loss: 9.1379\n",
      "Epoch [1/5], Step [520/25334], D_loss: 0.0202, G_loss: 11.3307\n",
      "Epoch [1/5], Step [521/25334], D_loss: 0.0143, G_loss: 11.5170\n",
      "Epoch [1/5], Step [522/25334], D_loss: 0.0191, G_loss: 19.3165\n",
      "Epoch [1/5], Step [523/25334], D_loss: 0.1768, G_loss: 6.2203\n",
      "Epoch [1/5], Step [524/25334], D_loss: 0.0529, G_loss: 7.9717\n",
      "Epoch [1/5], Step [525/25334], D_loss: 0.0084, G_loss: 18.5487\n",
      "Epoch [1/5], Step [526/25334], D_loss: 0.0160, G_loss: 15.1711\n",
      "Epoch [1/5], Step [527/25334], D_loss: 0.1021, G_loss: 8.9410\n",
      "Epoch [1/5], Step [528/25334], D_loss: 0.3060, G_loss: 5.3251\n",
      "Epoch [1/5], Step [529/25334], D_loss: 0.3414, G_loss: 13.3010\n",
      "Epoch [1/5], Step [530/25334], D_loss: 0.0132, G_loss: 14.6223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [531/25334], D_loss: 0.0140, G_loss: 19.8197\n",
      "Epoch [1/5], Step [532/25334], D_loss: 0.0174, G_loss: 14.4754\n",
      "Epoch [1/5], Step [533/25334], D_loss: 0.0688, G_loss: 10.0254\n",
      "Epoch [1/5], Step [534/25334], D_loss: 0.0212, G_loss: 13.9412\n",
      "Epoch [1/5], Step [535/25334], D_loss: 0.0175, G_loss: 8.0823\n",
      "Epoch [1/5], Step [536/25334], D_loss: 0.0171, G_loss: 21.2945\n",
      "Epoch [1/5], Step [537/25334], D_loss: 0.0211, G_loss: 9.3512\n",
      "Epoch [1/5], Step [538/25334], D_loss: 0.0169, G_loss: 22.5324\n",
      "Epoch [1/5], Step [539/25334], D_loss: 0.0197, G_loss: 10.6262\n",
      "Epoch [1/5], Step [540/25334], D_loss: 0.0187, G_loss: 26.4346\n",
      "Epoch [1/5], Step [541/25334], D_loss: 0.0614, G_loss: 6.4701\n",
      "Epoch [1/5], Step [542/25334], D_loss: 0.1134, G_loss: 7.5041\n",
      "Epoch [1/5], Step [543/25334], D_loss: 0.0105, G_loss: 11.8202\n",
      "Epoch [1/5], Step [544/25334], D_loss: 0.0535, G_loss: 10.8560\n",
      "Epoch [1/5], Step [545/25334], D_loss: 0.1732, G_loss: 6.8201\n",
      "Epoch [1/5], Step [546/25334], D_loss: 0.0163, G_loss: 11.7641\n",
      "Epoch [1/5], Step [547/25334], D_loss: 0.0219, G_loss: 10.0244\n",
      "Epoch [1/5], Step [548/25334], D_loss: 0.0213, G_loss: 15.2366\n",
      "Epoch [1/5], Step [549/25334], D_loss: 0.0101, G_loss: 11.3902\n",
      "Epoch [1/5], Step [550/25334], D_loss: 0.0193, G_loss: 22.4134\n",
      "Epoch [1/5], Step [551/25334], D_loss: 0.0428, G_loss: 12.1953\n",
      "Epoch [1/5], Step [552/25334], D_loss: 0.3649, G_loss: 6.9536\n",
      "Epoch [1/5], Step [553/25334], D_loss: 0.0857, G_loss: 8.7231\n",
      "Epoch [1/5], Step [554/25334], D_loss: 0.0282, G_loss: 35.8770\n",
      "Epoch [1/5], Step [555/25334], D_loss: 0.0131, G_loss: 12.2948\n",
      "Epoch [1/5], Step [556/25334], D_loss: 0.0348, G_loss: 10.5543\n",
      "Epoch [1/5], Step [557/25334], D_loss: 0.1894, G_loss: 5.1662\n",
      "Epoch [1/5], Step [558/25334], D_loss: 0.1524, G_loss: 7.0336\n",
      "Epoch [1/5], Step [559/25334], D_loss: 0.4696, G_loss: 5.8072\n",
      "Epoch [1/5], Step [560/25334], D_loss: 0.1182, G_loss: 10.1128\n",
      "Epoch [1/5], Step [561/25334], D_loss: 0.0157, G_loss: 17.9306\n",
      "Epoch [1/5], Step [562/25334], D_loss: 0.0143, G_loss: 11.4701\n",
      "Epoch [1/5], Step [563/25334], D_loss: 0.2040, G_loss: 12.3764\n",
      "Epoch [1/5], Step [564/25334], D_loss: 0.0280, G_loss: 9.8859\n",
      "Epoch [1/5], Step [565/25334], D_loss: 0.1086, G_loss: 22.9791\n",
      "Epoch [1/5], Step [566/25334], D_loss: 0.0509, G_loss: 23.3771\n",
      "Epoch [1/5], Step [567/25334], D_loss: 0.0678, G_loss: 8.4251\n",
      "Epoch [1/5], Step [568/25334], D_loss: 0.0315, G_loss: 8.0675\n",
      "Epoch [1/5], Step [569/25334], D_loss: 0.0402, G_loss: 20.3010\n",
      "Epoch [1/5], Step [570/25334], D_loss: 0.0910, G_loss: 8.6015\n",
      "Epoch [1/5], Step [571/25334], D_loss: 0.0714, G_loss: 21.4599\n",
      "Epoch [1/5], Step [572/25334], D_loss: 0.0463, G_loss: 11.3310\n",
      "Epoch [1/5], Step [573/25334], D_loss: 0.0136, G_loss: 22.2920\n",
      "Epoch [1/5], Step [574/25334], D_loss: 0.0110, G_loss: 11.4208\n",
      "Epoch [1/5], Step [575/25334], D_loss: 0.0068, G_loss: 12.2653\n",
      "Epoch [1/5], Step [576/25334], D_loss: 0.0114, G_loss: 15.3900\n",
      "Epoch [1/5], Step [577/25334], D_loss: 0.0209, G_loss: 11.9995\n",
      "Epoch [1/5], Step [578/25334], D_loss: 0.0159, G_loss: 11.8684\n",
      "Epoch [1/5], Step [579/25334], D_loss: 0.0476, G_loss: 7.2152\n",
      "Epoch [1/5], Step [580/25334], D_loss: 0.0493, G_loss: 34.3131\n",
      "Epoch [1/5], Step [581/25334], D_loss: 0.0064, G_loss: 19.2379\n",
      "Epoch [1/5], Step [582/25334], D_loss: 0.0089, G_loss: 14.1525\n",
      "Epoch [1/5], Step [583/25334], D_loss: 0.0097, G_loss: 15.7888\n",
      "Epoch [1/5], Step [584/25334], D_loss: 0.0648, G_loss: 12.2652\n",
      "Epoch [1/5], Step [585/25334], D_loss: 0.0506, G_loss: 8.1468\n",
      "Epoch [1/5], Step [586/25334], D_loss: 0.0271, G_loss: 15.7846\n",
      "Epoch [1/5], Step [587/25334], D_loss: 0.0157, G_loss: 28.2807\n",
      "Epoch [1/5], Step [588/25334], D_loss: 0.9997, G_loss: 6.8025\n",
      "Epoch [1/5], Step [589/25334], D_loss: 0.8244, G_loss: 9.4890\n",
      "Epoch [1/5], Step [590/25334], D_loss: 0.0493, G_loss: 22.1050\n",
      "Epoch [1/5], Step [591/25334], D_loss: 0.1414, G_loss: 19.3644\n",
      "Epoch [1/5], Step [592/25334], D_loss: 0.0414, G_loss: 12.4677\n",
      "Epoch [1/5], Step [593/25334], D_loss: 0.4172, G_loss: 7.3457\n",
      "Epoch [1/5], Step [594/25334], D_loss: 0.0360, G_loss: 18.8206\n",
      "Epoch [1/5], Step [595/25334], D_loss: 0.2412, G_loss: 10.5716\n",
      "Epoch [1/5], Step [596/25334], D_loss: 0.0133, G_loss: 19.5276\n",
      "Epoch [1/5], Step [597/25334], D_loss: 0.1333, G_loss: 7.2646\n",
      "Epoch [1/5], Step [598/25334], D_loss: 0.0628, G_loss: 7.0745\n",
      "Epoch [1/5], Step [599/25334], D_loss: 0.0274, G_loss: 13.9562\n",
      "Epoch [1/5], Step [600/25334], D_loss: 0.0100, G_loss: 22.0636\n",
      "Epoch [1/5], Step [601/25334], D_loss: 0.0349, G_loss: 11.3057\n",
      "Epoch [1/5], Step [602/25334], D_loss: 0.0240, G_loss: 43.0482\n",
      "Epoch [1/5], Step [603/25334], D_loss: 0.2840, G_loss: 10.6926\n",
      "Epoch [1/5], Step [604/25334], D_loss: 0.1337, G_loss: 8.1896\n",
      "Epoch [1/5], Step [605/25334], D_loss: 0.5234, G_loss: 7.0745\n",
      "Epoch [1/5], Step [606/25334], D_loss: 0.0235, G_loss: 10.9811\n",
      "Epoch [1/5], Step [607/25334], D_loss: 0.1452, G_loss: 9.9626\n",
      "Epoch [1/5], Step [608/25334], D_loss: 0.6970, G_loss: 7.6056\n",
      "Epoch [1/5], Step [609/25334], D_loss: 0.2262, G_loss: 7.6926\n",
      "Epoch [1/5], Step [610/25334], D_loss: 0.0191, G_loss: 14.2234\n",
      "Epoch [1/5], Step [611/25334], D_loss: 0.0506, G_loss: 7.1874\n",
      "Epoch [1/5], Step [612/25334], D_loss: 0.0178, G_loss: 20.1698\n",
      "Epoch [1/5], Step [613/25334], D_loss: 0.1474, G_loss: 8.5213\n",
      "Epoch [1/5], Step [614/25334], D_loss: 0.0337, G_loss: 13.3963\n",
      "Epoch [1/5], Step [615/25334], D_loss: 0.3763, G_loss: 7.2357\n",
      "Epoch [1/5], Step [616/25334], D_loss: 0.0081, G_loss: 17.0340\n",
      "Epoch [1/5], Step [617/25334], D_loss: 0.0246, G_loss: 8.9024\n",
      "Epoch [1/5], Step [618/25334], D_loss: 0.0451, G_loss: 10.3511\n",
      "Epoch [1/5], Step [619/25334], D_loss: 0.0124, G_loss: 12.9083\n",
      "Epoch [1/5], Step [620/25334], D_loss: 0.0101, G_loss: 12.7898\n",
      "Epoch [1/5], Step [621/25334], D_loss: 0.0255, G_loss: 9.7486\n",
      "Epoch [1/5], Step [622/25334], D_loss: 0.0730, G_loss: 7.0831\n",
      "Epoch [1/5], Step [623/25334], D_loss: 0.1091, G_loss: 7.6200\n",
      "Epoch [1/5], Step [624/25334], D_loss: 0.0496, G_loss: 8.3099\n",
      "Epoch [1/5], Step [625/25334], D_loss: 0.0292, G_loss: 7.8980\n",
      "Epoch [1/5], Step [626/25334], D_loss: 0.0155, G_loss: 35.1926\n",
      "Epoch [1/5], Step [627/25334], D_loss: 0.0287, G_loss: 16.5674\n",
      "Epoch [1/5], Step [628/25334], D_loss: 0.0549, G_loss: 40.8946\n",
      "Epoch [1/5], Step [629/25334], D_loss: 0.2425, G_loss: 9.9180\n",
      "Epoch [1/5], Step [630/25334], D_loss: 0.0169, G_loss: 20.9445\n",
      "Epoch [1/5], Step [631/25334], D_loss: 0.0567, G_loss: 8.0382\n",
      "Epoch [1/5], Step [632/25334], D_loss: 0.0079, G_loss: 11.1544\n",
      "Epoch [1/5], Step [633/25334], D_loss: 0.0172, G_loss: 16.0897\n",
      "Epoch [1/5], Step [634/25334], D_loss: 0.0070, G_loss: 18.7446\n",
      "Epoch [1/5], Step [635/25334], D_loss: 0.0328, G_loss: 5.9243\n",
      "Epoch [1/5], Step [636/25334], D_loss: 0.0757, G_loss: 6.6027\n",
      "Epoch [1/5], Step [637/25334], D_loss: 0.0159, G_loss: 20.8633\n",
      "Epoch [1/5], Step [638/25334], D_loss: 0.0726, G_loss: 9.5547\n",
      "Epoch [1/5], Step [639/25334], D_loss: 0.0080, G_loss: 17.7372\n",
      "Epoch [1/5], Step [640/25334], D_loss: 0.0064, G_loss: 16.9807\n",
      "Epoch [1/5], Step [641/25334], D_loss: 0.0246, G_loss: 11.8511\n",
      "Epoch [1/5], Step [642/25334], D_loss: 0.0087, G_loss: 14.7253\n",
      "Epoch [1/5], Step [643/25334], D_loss: 0.0177, G_loss: 15.2731\n",
      "Epoch [1/5], Step [644/25334], D_loss: 0.0876, G_loss: 10.1771\n",
      "Epoch [1/5], Step [645/25334], D_loss: 0.0931, G_loss: 7.4139\n",
      "Epoch [1/5], Step [646/25334], D_loss: 0.1224, G_loss: 6.7507\n",
      "Epoch [1/5], Step [647/25334], D_loss: 0.0106, G_loss: 21.9229\n",
      "Epoch [1/5], Step [648/25334], D_loss: 0.0070, G_loss: 16.0836\n",
      "Epoch [1/5], Step [649/25334], D_loss: 0.0081, G_loss: 13.2480\n",
      "Epoch [1/5], Step [650/25334], D_loss: 0.0338, G_loss: 12.6677\n",
      "Epoch [1/5], Step [651/25334], D_loss: 0.0430, G_loss: 9.4983\n",
      "Epoch [1/5], Step [652/25334], D_loss: 0.0048, G_loss: 18.5266\n",
      "Epoch [1/5], Step [653/25334], D_loss: 0.0999, G_loss: 5.6029\n",
      "Epoch [1/5], Step [654/25334], D_loss: 0.0689, G_loss: 7.9330\n",
      "Epoch [1/5], Step [655/25334], D_loss: 0.0248, G_loss: 9.9066\n",
      "Epoch [1/5], Step [656/25334], D_loss: 0.0059, G_loss: 11.4892\n",
      "Epoch [1/5], Step [657/25334], D_loss: 0.0528, G_loss: 8.8266\n",
      "Epoch [1/5], Step [658/25334], D_loss: 0.0048, G_loss: 16.1336\n",
      "Epoch [1/5], Step [659/25334], D_loss: 0.0044, G_loss: 14.9365\n",
      "Epoch [1/5], Step [660/25334], D_loss: 0.0078, G_loss: 16.2718\n",
      "Epoch [1/5], Step [661/25334], D_loss: 0.0143, G_loss: 24.2908\n",
      "Epoch [1/5], Step [662/25334], D_loss: 0.0161, G_loss: 7.9468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [663/25334], D_loss: 0.0557, G_loss: 7.7283\n",
      "Epoch [1/5], Step [664/25334], D_loss: 0.0309, G_loss: 7.0874\n",
      "Epoch [1/5], Step [665/25334], D_loss: 0.5287, G_loss: 6.5886\n",
      "Epoch [1/5], Step [666/25334], D_loss: 0.1043, G_loss: 8.1978\n",
      "Epoch [1/5], Step [667/25334], D_loss: 0.0099, G_loss: 10.3656\n",
      "Epoch [1/5], Step [668/25334], D_loss: 0.1733, G_loss: 6.9648\n",
      "Epoch [1/5], Step [669/25334], D_loss: 0.0104, G_loss: 20.1014\n",
      "Epoch [1/5], Step [670/25334], D_loss: 0.0714, G_loss: 12.1616\n",
      "Epoch [1/5], Step [671/25334], D_loss: 0.0482, G_loss: 8.1864\n",
      "Epoch [1/5], Step [672/25334], D_loss: 0.0197, G_loss: 8.4603\n",
      "Epoch [1/5], Step [673/25334], D_loss: 0.0122, G_loss: 13.0085\n",
      "Epoch [1/5], Step [674/25334], D_loss: 0.0315, G_loss: 8.1363\n",
      "Epoch [1/5], Step [675/25334], D_loss: 0.1612, G_loss: 7.5543\n",
      "Epoch [1/5], Step [676/25334], D_loss: 0.0164, G_loss: 29.1799\n",
      "Epoch [1/5], Step [677/25334], D_loss: 0.0090, G_loss: 15.2098\n",
      "Epoch [1/5], Step [678/25334], D_loss: 0.3407, G_loss: 7.1332\n",
      "Epoch [1/5], Step [679/25334], D_loss: 0.2173, G_loss: 8.8303\n",
      "Epoch [1/5], Step [680/25334], D_loss: 0.1923, G_loss: 6.4680\n",
      "Epoch [1/5], Step [681/25334], D_loss: 0.0295, G_loss: 7.3800\n",
      "Epoch [1/5], Step [682/25334], D_loss: 0.0128, G_loss: 10.5692\n",
      "Epoch [1/5], Step [683/25334], D_loss: 0.0236, G_loss: 13.3604\n",
      "Epoch [1/5], Step [684/25334], D_loss: 0.0159, G_loss: 11.8745\n",
      "Epoch [1/5], Step [685/25334], D_loss: 0.0156, G_loss: 7.7818\n",
      "Epoch [1/5], Step [686/25334], D_loss: 0.1439, G_loss: 11.2908\n",
      "Epoch [1/5], Step [687/25334], D_loss: 0.3166, G_loss: 6.9119\n",
      "Epoch [1/5], Step [688/25334], D_loss: 0.0151, G_loss: 18.1316\n",
      "Epoch [1/5], Step [689/25334], D_loss: 0.0187, G_loss: 26.7016\n",
      "Epoch [1/5], Step [690/25334], D_loss: 0.6041, G_loss: 8.2201\n",
      "Epoch [1/5], Step [691/25334], D_loss: 0.2714, G_loss: 7.1717\n",
      "Epoch [1/5], Step [692/25334], D_loss: 0.1620, G_loss: 12.0697\n",
      "Epoch [1/5], Step [693/25334], D_loss: 0.1256, G_loss: 10.0635\n",
      "Epoch [1/5], Step [694/25334], D_loss: 0.2045, G_loss: 13.6484\n",
      "Epoch [1/5], Step [695/25334], D_loss: 0.2401, G_loss: 13.5728\n",
      "Epoch [1/5], Step [696/25334], D_loss: 0.0250, G_loss: 9.7446\n",
      "Epoch [1/5], Step [697/25334], D_loss: 0.3385, G_loss: 6.2651\n",
      "Epoch [1/5], Step [698/25334], D_loss: 0.0583, G_loss: 10.8361\n",
      "Epoch [1/5], Step [699/25334], D_loss: 0.0476, G_loss: 10.0165\n",
      "Epoch [1/5], Step [700/25334], D_loss: 0.0143, G_loss: 13.1195\n",
      "Epoch [1/5], Step [701/25334], D_loss: 0.0313, G_loss: 9.8037\n",
      "Epoch [1/5], Step [702/25334], D_loss: 0.0091, G_loss: 17.4278\n",
      "Epoch [1/5], Step [703/25334], D_loss: 0.0194, G_loss: 17.0413\n",
      "Epoch [1/5], Step [704/25334], D_loss: 0.0636, G_loss: 10.0002\n",
      "Epoch [1/5], Step [705/25334], D_loss: 0.0113, G_loss: 19.3601\n",
      "Epoch [1/5], Step [706/25334], D_loss: 0.2884, G_loss: 7.5766\n",
      "Epoch [1/5], Step [707/25334], D_loss: 0.0114, G_loss: 10.9985\n",
      "Epoch [1/5], Step [708/25334], D_loss: 0.0163, G_loss: 12.6110\n",
      "Epoch [1/5], Step [709/25334], D_loss: 0.0498, G_loss: 15.4490\n",
      "Epoch [1/5], Step [710/25334], D_loss: 0.0321, G_loss: 8.4034\n",
      "Epoch [1/5], Step [711/25334], D_loss: 0.0553, G_loss: 7.2143\n",
      "Epoch [1/5], Step [712/25334], D_loss: 0.0305, G_loss: 33.6014\n",
      "Epoch [1/5], Step [713/25334], D_loss: 0.1117, G_loss: 7.8824\n",
      "Epoch [1/5], Step [714/25334], D_loss: 0.1877, G_loss: 9.7529\n",
      "Epoch [1/5], Step [715/25334], D_loss: 0.1636, G_loss: 9.1189\n",
      "Epoch [1/5], Step [716/25334], D_loss: 0.6178, G_loss: 11.3546\n",
      "Epoch [1/5], Step [717/25334], D_loss: 0.0965, G_loss: 10.3438\n",
      "Epoch [1/5], Step [718/25334], D_loss: 0.0258, G_loss: 13.7949\n",
      "Epoch [1/5], Step [719/25334], D_loss: 0.2018, G_loss: 7.1438\n",
      "Epoch [1/5], Step [720/25334], D_loss: 0.1556, G_loss: 9.8012\n",
      "Epoch [1/5], Step [721/25334], D_loss: 0.0166, G_loss: 8.1141\n",
      "Epoch [1/5], Step [722/25334], D_loss: 0.0240, G_loss: 11.4144\n",
      "Epoch [1/5], Step [723/25334], D_loss: 0.3183, G_loss: 10.3391\n",
      "Epoch [1/5], Step [724/25334], D_loss: 0.0261, G_loss: 12.1808\n",
      "Epoch [1/5], Step [725/25334], D_loss: 0.1840, G_loss: 6.2868\n",
      "Epoch [1/5], Step [726/25334], D_loss: 0.0195, G_loss: 20.8055\n",
      "Epoch [1/5], Step [727/25334], D_loss: 0.0492, G_loss: 10.3631\n",
      "Epoch [1/5], Step [728/25334], D_loss: 0.0187, G_loss: 18.8496\n",
      "Epoch [1/5], Step [729/25334], D_loss: 0.0169, G_loss: 8.7321\n",
      "Epoch [1/5], Step [730/25334], D_loss: 0.0097, G_loss: 13.2982\n",
      "Epoch [1/5], Step [731/25334], D_loss: 0.0073, G_loss: 16.1465\n",
      "Epoch [1/5], Step [732/25334], D_loss: 0.0354, G_loss: 6.6793\n",
      "Epoch [1/5], Step [733/25334], D_loss: 0.0168, G_loss: 13.0268\n",
      "Epoch [1/5], Step [734/25334], D_loss: 0.0226, G_loss: 9.2993\n",
      "Epoch [1/5], Step [735/25334], D_loss: 0.1291, G_loss: 6.1093\n",
      "Epoch [1/5], Step [736/25334], D_loss: 0.0343, G_loss: 9.7947\n",
      "Epoch [1/5], Step [737/25334], D_loss: 0.0063, G_loss: 11.5810\n",
      "Epoch [1/5], Step [738/25334], D_loss: 0.0070, G_loss: 14.9971\n",
      "Epoch [1/5], Step [739/25334], D_loss: 0.0079, G_loss: 12.6830\n",
      "Epoch [1/5], Step [740/25334], D_loss: 0.0121, G_loss: 7.5755\n",
      "Epoch [1/5], Step [741/25334], D_loss: 0.4890, G_loss: 8.8579\n",
      "Epoch [1/5], Step [742/25334], D_loss: 0.0460, G_loss: 14.5522\n",
      "Epoch [1/5], Step [743/25334], D_loss: 0.3368, G_loss: 8.1421\n",
      "Epoch [1/5], Step [744/25334], D_loss: 0.0122, G_loss: 19.3676\n",
      "Epoch [1/5], Step [745/25334], D_loss: 0.0076, G_loss: 12.7617\n",
      "Epoch [1/5], Step [746/25334], D_loss: 0.0058, G_loss: 15.6134\n",
      "Epoch [1/5], Step [747/25334], D_loss: 0.0157, G_loss: 14.9749\n",
      "Epoch [1/5], Step [748/25334], D_loss: 0.0084, G_loss: 10.8154\n",
      "Epoch [1/5], Step [749/25334], D_loss: 0.0343, G_loss: 9.6921\n",
      "Epoch [1/5], Step [750/25334], D_loss: 0.0400, G_loss: 12.2884\n",
      "Epoch [1/5], Step [751/25334], D_loss: 0.0806, G_loss: 7.5475\n",
      "Epoch [1/5], Step [752/25334], D_loss: 0.0790, G_loss: 6.0171\n",
      "Epoch [1/5], Step [753/25334], D_loss: 0.0054, G_loss: 12.7661\n",
      "Epoch [1/5], Step [754/25334], D_loss: 0.0174, G_loss: 9.1637\n",
      "Epoch [1/5], Step [755/25334], D_loss: 0.0390, G_loss: 9.2096\n",
      "Epoch [1/5], Step [756/25334], D_loss: 0.0121, G_loss: 15.6340\n",
      "Epoch [1/5], Step [757/25334], D_loss: 0.5865, G_loss: 6.4098\n",
      "Epoch [1/5], Step [758/25334], D_loss: 0.0187, G_loss: 13.8579\n",
      "Epoch [1/5], Step [759/25334], D_loss: 0.0102, G_loss: 18.0032\n",
      "Epoch [1/5], Step [760/25334], D_loss: 0.0173, G_loss: 12.7000\n",
      "Epoch [1/5], Step [761/25334], D_loss: 0.0496, G_loss: 11.7336\n",
      "Epoch [1/5], Step [762/25334], D_loss: 0.9340, G_loss: 7.7105\n",
      "Epoch [1/5], Step [763/25334], D_loss: 0.0900, G_loss: 6.9744\n",
      "Epoch [1/5], Step [764/25334], D_loss: 0.0415, G_loss: 36.6439\n",
      "Epoch [1/5], Step [765/25334], D_loss: 0.0125, G_loss: 10.5490\n",
      "Epoch [1/5], Step [766/25334], D_loss: 0.0193, G_loss: 7.1404\n",
      "Epoch [1/5], Step [767/25334], D_loss: 0.0145, G_loss: 15.5494\n",
      "Epoch [1/5], Step [768/25334], D_loss: 0.0211, G_loss: 17.0445\n",
      "Epoch [1/5], Step [769/25334], D_loss: 0.3344, G_loss: 5.8748\n",
      "Epoch [1/5], Step [770/25334], D_loss: 0.0343, G_loss: 8.8474\n",
      "Epoch [1/5], Step [771/25334], D_loss: 0.0502, G_loss: 12.6293\n",
      "Epoch [1/5], Step [772/25334], D_loss: 0.0893, G_loss: 9.1894\n",
      "Epoch [1/5], Step [773/25334], D_loss: 0.0110, G_loss: 12.9707\n",
      "Epoch [1/5], Step [774/25334], D_loss: 0.0227, G_loss: 8.8171\n",
      "Epoch [1/5], Step [775/25334], D_loss: 0.0134, G_loss: 8.9407\n",
      "Epoch [1/5], Step [776/25334], D_loss: 0.0094, G_loss: 19.3674\n",
      "Epoch [1/5], Step [777/25334], D_loss: 0.9456, G_loss: 4.6014\n",
      "Epoch [1/5], Step [778/25334], D_loss: 0.0352, G_loss: 17.3430\n",
      "Epoch [1/5], Step [779/25334], D_loss: 0.0624, G_loss: 12.7893\n",
      "Epoch [1/5], Step [780/25334], D_loss: 0.2565, G_loss: 8.6651\n",
      "Epoch [1/5], Step [781/25334], D_loss: 0.0169, G_loss: 22.8611\n",
      "Epoch [1/5], Step [782/25334], D_loss: 0.0212, G_loss: 10.4509\n",
      "Epoch [1/5], Step [783/25334], D_loss: 0.0368, G_loss: 14.0560\n",
      "Epoch [1/5], Step [784/25334], D_loss: 0.0163, G_loss: 9.2115\n",
      "Epoch [1/5], Step [785/25334], D_loss: 0.0070, G_loss: 11.4856\n",
      "Epoch [1/5], Step [786/25334], D_loss: 0.1203, G_loss: 6.8296\n",
      "Epoch [1/5], Step [787/25334], D_loss: 0.0374, G_loss: 20.4267\n",
      "Epoch [1/5], Step [788/25334], D_loss: 0.0077, G_loss: 13.3645\n",
      "Epoch [1/5], Step [789/25334], D_loss: 0.0099, G_loss: 19.1552\n",
      "Epoch [1/5], Step [790/25334], D_loss: 0.0161, G_loss: 13.2939\n",
      "Epoch [1/5], Step [791/25334], D_loss: 0.3765, G_loss: 4.1944\n",
      "Epoch [1/5], Step [792/25334], D_loss: 0.0122, G_loss: 25.5703\n",
      "Epoch [1/5], Step [793/25334], D_loss: 0.0050, G_loss: 16.4044\n",
      "Epoch [1/5], Step [794/25334], D_loss: 0.0180, G_loss: 13.2168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [795/25334], D_loss: 0.0068, G_loss: 11.4988\n",
      "Epoch [1/5], Step [796/25334], D_loss: 0.0060, G_loss: 18.1303\n",
      "Epoch [1/5], Step [797/25334], D_loss: 0.3019, G_loss: 10.5034\n",
      "Epoch [1/5], Step [798/25334], D_loss: 0.0067, G_loss: 11.8295\n",
      "Epoch [1/5], Step [799/25334], D_loss: 0.0163, G_loss: 26.2967\n",
      "Epoch [1/5], Step [800/25334], D_loss: 0.0085, G_loss: 9.3145\n",
      "Epoch [1/5], Step [801/25334], D_loss: 0.0081, G_loss: 13.2557\n",
      "Epoch [1/5], Step [802/25334], D_loss: 0.0068, G_loss: 15.5235\n",
      "Epoch [1/5], Step [803/25334], D_loss: 0.0248, G_loss: 6.7190\n",
      "Epoch [1/5], Step [804/25334], D_loss: 0.0086, G_loss: 11.6254\n",
      "Epoch [1/5], Step [805/25334], D_loss: 0.0051, G_loss: 16.9124\n",
      "Epoch [1/5], Step [806/25334], D_loss: 0.0061, G_loss: 15.8915\n",
      "Epoch [1/5], Step [807/25334], D_loss: 0.0417, G_loss: 39.2384\n",
      "Epoch [1/5], Step [808/25334], D_loss: 0.0241, G_loss: 7.2870\n",
      "Epoch [1/5], Step [809/25334], D_loss: 0.0077, G_loss: 13.5596\n",
      "Epoch [1/5], Step [810/25334], D_loss: 0.0528, G_loss: 6.0527\n",
      "Epoch [1/5], Step [811/25334], D_loss: 0.0274, G_loss: 17.9156\n",
      "Epoch [1/5], Step [812/25334], D_loss: 0.1289, G_loss: 48.6468\n",
      "Epoch [1/5], Step [813/25334], D_loss: 0.0165, G_loss: 12.1222\n",
      "Epoch [1/5], Step [814/25334], D_loss: 0.0140, G_loss: 9.3455\n",
      "Epoch [1/5], Step [815/25334], D_loss: 0.0079, G_loss: 14.9755\n",
      "Epoch [1/5], Step [816/25334], D_loss: 0.0253, G_loss: 8.4081\n",
      "Epoch [1/5], Step [817/25334], D_loss: 0.0108, G_loss: 17.7924\n",
      "Epoch [1/5], Step [818/25334], D_loss: 0.0771, G_loss: 5.4338\n",
      "Epoch [1/5], Step [819/25334], D_loss: 0.0038, G_loss: 16.7805\n",
      "Epoch [1/5], Step [820/25334], D_loss: 0.0082, G_loss: 18.1307\n",
      "Epoch [1/5], Step [821/25334], D_loss: 0.0080, G_loss: 22.9705\n",
      "Epoch [1/5], Step [822/25334], D_loss: 0.0102, G_loss: 26.0621\n",
      "Epoch [1/5], Step [823/25334], D_loss: 0.0086, G_loss: 10.1896\n",
      "Epoch [1/5], Step [824/25334], D_loss: 0.0096, G_loss: 9.2656\n",
      "Epoch [1/5], Step [825/25334], D_loss: 0.0572, G_loss: 11.9025\n",
      "Epoch [1/5], Step [826/25334], D_loss: 0.2292, G_loss: 5.1864\n",
      "Epoch [1/5], Step [827/25334], D_loss: 0.0038, G_loss: 14.1107\n",
      "Epoch [1/5], Step [828/25334], D_loss: 0.0171, G_loss: 12.5535\n",
      "Epoch [1/5], Step [829/25334], D_loss: 0.0084, G_loss: 11.9554\n",
      "Epoch [1/5], Step [830/25334], D_loss: 0.0113, G_loss: 16.6525\n",
      "Epoch [1/5], Step [831/25334], D_loss: 0.0152, G_loss: 25.3335\n",
      "Epoch [1/5], Step [832/25334], D_loss: 0.0036, G_loss: 12.5188\n",
      "Epoch [1/5], Step [833/25334], D_loss: 0.0036, G_loss: 19.1315\n",
      "Epoch [1/5], Step [834/25334], D_loss: 0.0075, G_loss: 12.6642\n",
      "Epoch [1/5], Step [835/25334], D_loss: 0.3901, G_loss: 8.6782\n",
      "Epoch [1/5], Step [836/25334], D_loss: 0.0037, G_loss: 14.6822\n",
      "Epoch [1/5], Step [837/25334], D_loss: 0.0089, G_loss: 15.7449\n",
      "Epoch [1/5], Step [838/25334], D_loss: 0.4778, G_loss: 8.3747\n",
      "Epoch [1/5], Step [839/25334], D_loss: 0.0506, G_loss: 27.6996\n",
      "Epoch [1/5], Step [840/25334], D_loss: 0.6580, G_loss: 6.0226\n",
      "Epoch [1/5], Step [841/25334], D_loss: 0.0201, G_loss: 9.5896\n",
      "Epoch [1/5], Step [842/25334], D_loss: 0.0216, G_loss: 10.8654\n",
      "Epoch [1/5], Step [843/25334], D_loss: 0.0224, G_loss: 15.9364\n",
      "Epoch [1/5], Step [844/25334], D_loss: 0.0631, G_loss: 10.7910\n",
      "Epoch [1/5], Step [845/25334], D_loss: 0.0170, G_loss: 12.1830\n",
      "Epoch [1/5], Step [846/25334], D_loss: 0.1942, G_loss: 11.8921\n",
      "Epoch [1/5], Step [847/25334], D_loss: 0.1044, G_loss: 6.9949\n",
      "Epoch [1/5], Step [848/25334], D_loss: 0.0112, G_loss: 7.2069\n",
      "Epoch [1/5], Step [849/25334], D_loss: 0.0449, G_loss: 23.6941\n",
      "Epoch [1/5], Step [850/25334], D_loss: 0.0089, G_loss: 17.9467\n",
      "Epoch [1/5], Step [851/25334], D_loss: 0.0632, G_loss: 8.9929\n",
      "Epoch [1/5], Step [852/25334], D_loss: 0.0540, G_loss: 10.1759\n",
      "Epoch [1/5], Step [853/25334], D_loss: 0.1937, G_loss: 5.4590\n",
      "Epoch [1/5], Step [854/25334], D_loss: 0.0064, G_loss: 13.8833\n",
      "Epoch [1/5], Step [855/25334], D_loss: 0.0075, G_loss: 14.6207\n",
      "Epoch [1/5], Step [856/25334], D_loss: 0.3766, G_loss: 7.3122\n",
      "Epoch [1/5], Step [857/25334], D_loss: 0.0128, G_loss: 14.9263\n",
      "Epoch [1/5], Step [858/25334], D_loss: 0.0966, G_loss: 9.8220\n",
      "Epoch [1/5], Step [859/25334], D_loss: 0.0449, G_loss: 7.9699\n",
      "Epoch [1/5], Step [860/25334], D_loss: 0.0097, G_loss: 14.8515\n",
      "Epoch [1/5], Step [861/25334], D_loss: 0.0065, G_loss: 9.9026\n",
      "Epoch [1/5], Step [862/25334], D_loss: 0.0838, G_loss: 9.1387\n",
      "Epoch [1/5], Step [863/25334], D_loss: 0.0258, G_loss: 7.4367\n",
      "Epoch [1/5], Step [864/25334], D_loss: 0.0054, G_loss: 23.2060\n",
      "Epoch [1/5], Step [865/25334], D_loss: 0.0027, G_loss: 13.2775\n",
      "Epoch [1/5], Step [866/25334], D_loss: 0.0261, G_loss: 9.2927\n",
      "Epoch [1/5], Step [867/25334], D_loss: 0.0087, G_loss: 10.1808\n",
      "Epoch [1/5], Step [868/25334], D_loss: 0.0151, G_loss: 14.1488\n",
      "Epoch [1/5], Step [869/25334], D_loss: 0.0049, G_loss: 11.5317\n",
      "Epoch [1/5], Step [870/25334], D_loss: 0.0121, G_loss: 7.8457\n",
      "Epoch [1/5], Step [871/25334], D_loss: 0.0170, G_loss: 38.3142\n",
      "Epoch [1/5], Step [872/25334], D_loss: 0.0039, G_loss: 11.5769\n",
      "Epoch [1/5], Step [873/25334], D_loss: 0.0321, G_loss: 7.7900\n",
      "Epoch [1/5], Step [874/25334], D_loss: 0.0197, G_loss: 14.1390\n",
      "Epoch [1/5], Step [875/25334], D_loss: 0.0019, G_loss: 14.6312\n",
      "Epoch [1/5], Step [876/25334], D_loss: 0.0449, G_loss: 9.5552\n",
      "Epoch [1/5], Step [877/25334], D_loss: 0.0028, G_loss: 13.8059\n",
      "Epoch [1/5], Step [878/25334], D_loss: 0.0045, G_loss: 13.2209\n",
      "Epoch [1/5], Step [879/25334], D_loss: 0.0073, G_loss: 13.2183\n",
      "Epoch [1/5], Step [880/25334], D_loss: 0.0048, G_loss: 13.4790\n",
      "Epoch [1/5], Step [881/25334], D_loss: 0.0055, G_loss: 13.5961\n",
      "Epoch [1/5], Step [882/25334], D_loss: 0.0058, G_loss: 15.6609\n",
      "Epoch [1/5], Step [883/25334], D_loss: 0.0869, G_loss: 5.4054\n",
      "Epoch [1/5], Step [884/25334], D_loss: 0.0034, G_loss: 13.4007\n",
      "Epoch [1/5], Step [885/25334], D_loss: 0.1134, G_loss: 7.3734\n",
      "Epoch [1/5], Step [886/25334], D_loss: 0.0330, G_loss: 7.8493\n",
      "Epoch [1/5], Step [887/25334], D_loss: 0.0275, G_loss: 6.8475\n",
      "Epoch [1/5], Step [888/25334], D_loss: 0.0025, G_loss: 16.2630\n",
      "Epoch [1/5], Step [889/25334], D_loss: 0.1141, G_loss: 5.2062\n",
      "Epoch [1/5], Step [890/25334], D_loss: 0.0048, G_loss: 13.8560\n",
      "Epoch [1/5], Step [891/25334], D_loss: 0.0141, G_loss: 9.1155\n",
      "Epoch [1/5], Step [892/25334], D_loss: 0.0044, G_loss: 18.8935\n",
      "Epoch [1/5], Step [893/25334], D_loss: 0.1026, G_loss: 8.6979\n",
      "Epoch [1/5], Step [894/25334], D_loss: 0.0028, G_loss: 13.2574\n",
      "Epoch [1/5], Step [895/25334], D_loss: 0.0025, G_loss: 12.5268\n",
      "Epoch [1/5], Step [896/25334], D_loss: 0.0056, G_loss: 15.8431\n",
      "Epoch [1/5], Step [897/25334], D_loss: 0.0605, G_loss: 5.8407\n",
      "Epoch [1/5], Step [898/25334], D_loss: 0.0139, G_loss: 14.7596\n",
      "Epoch [1/5], Step [899/25334], D_loss: 0.0042, G_loss: 18.8472\n",
      "Epoch [1/5], Step [900/25334], D_loss: 0.0353, G_loss: 5.9968\n",
      "Epoch [1/5], Step [901/25334], D_loss: 0.0041, G_loss: 15.9075\n",
      "Epoch [1/5], Step [902/25334], D_loss: 0.0073, G_loss: 22.5487\n",
      "Epoch [1/5], Step [903/25334], D_loss: 0.0513, G_loss: 5.8789\n",
      "Epoch [1/5], Step [904/25334], D_loss: 0.0181, G_loss: 34.9991\n",
      "Epoch [1/5], Step [905/25334], D_loss: 0.0458, G_loss: 12.2075\n",
      "Epoch [1/5], Step [906/25334], D_loss: 0.0107, G_loss: 11.5896\n",
      "Epoch [1/5], Step [907/25334], D_loss: 0.0484, G_loss: 47.0574\n",
      "Epoch [1/5], Step [908/25334], D_loss: 0.0142, G_loss: 8.9246\n",
      "Epoch [1/5], Step [909/25334], D_loss: 0.0159, G_loss: 12.8103\n",
      "Epoch [1/5], Step [910/25334], D_loss: 0.0234, G_loss: 9.0335\n",
      "Epoch [1/5], Step [911/25334], D_loss: 0.0172, G_loss: 30.5399\n",
      "Epoch [1/5], Step [912/25334], D_loss: 0.0073, G_loss: 13.8962\n",
      "Epoch [1/5], Step [913/25334], D_loss: 0.0044, G_loss: 14.2144\n",
      "Epoch [1/5], Step [914/25334], D_loss: 0.0092, G_loss: 12.6068\n",
      "Epoch [1/5], Step [915/25334], D_loss: 0.0542, G_loss: 6.7712\n",
      "Epoch [1/5], Step [916/25334], D_loss: 0.0537, G_loss: 5.4019\n",
      "Epoch [1/5], Step [917/25334], D_loss: 0.0078, G_loss: 9.1260\n",
      "Epoch [1/5], Step [918/25334], D_loss: 0.0919, G_loss: 6.2825\n",
      "Epoch [1/5], Step [919/25334], D_loss: 0.0042, G_loss: 16.5766\n",
      "Epoch [1/5], Step [920/25334], D_loss: 0.0046, G_loss: 8.6980\n",
      "Epoch [1/5], Step [921/25334], D_loss: 0.0040, G_loss: 11.6164\n",
      "Epoch [1/5], Step [922/25334], D_loss: 0.0122, G_loss: 21.4058\n",
      "Epoch [1/5], Step [923/25334], D_loss: 0.2492, G_loss: 6.1306\n",
      "Epoch [1/5], Step [924/25334], D_loss: 0.0035, G_loss: 16.6450\n",
      "Epoch [1/5], Step [925/25334], D_loss: 0.0097, G_loss: 27.1238\n",
      "Epoch [1/5], Step [926/25334], D_loss: 0.0062, G_loss: 14.4110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [927/25334], D_loss: 0.0037, G_loss: 16.0770\n",
      "Epoch [1/5], Step [928/25334], D_loss: 0.0040, G_loss: 17.5973\n",
      "Epoch [1/5], Step [929/25334], D_loss: 0.0110, G_loss: 11.6306\n",
      "Epoch [1/5], Step [930/25334], D_loss: 0.0039, G_loss: 15.6204\n",
      "Epoch [1/5], Step [931/25334], D_loss: 0.0316, G_loss: 9.9939\n",
      "Epoch [1/5], Step [932/25334], D_loss: 0.0051, G_loss: 18.2558\n",
      "Epoch [1/5], Step [933/25334], D_loss: 0.0027, G_loss: 12.2481\n",
      "Epoch [1/5], Step [934/25334], D_loss: 0.0024, G_loss: 14.6845\n",
      "Epoch [1/5], Step [935/25334], D_loss: 0.0027, G_loss: 11.8380\n",
      "Epoch [1/5], Step [936/25334], D_loss: 0.0051, G_loss: 11.2928\n",
      "Epoch [1/5], Step [937/25334], D_loss: 0.0089, G_loss: 35.1408\n",
      "Epoch [1/5], Step [938/25334], D_loss: 0.1656, G_loss: 7.7669\n",
      "Epoch [1/5], Step [939/25334], D_loss: 0.0099, G_loss: 10.6555\n",
      "Epoch [1/5], Step [940/25334], D_loss: 0.0050, G_loss: 14.5952\n",
      "Epoch [1/5], Step [941/25334], D_loss: 0.0456, G_loss: 8.5349\n",
      "Epoch [1/5], Step [942/25334], D_loss: 0.0325, G_loss: 7.2317\n",
      "Epoch [1/5], Step [943/25334], D_loss: 0.0682, G_loss: 6.5578\n",
      "Epoch [1/5], Step [944/25334], D_loss: 0.0047, G_loss: 14.6631\n",
      "Epoch [1/5], Step [945/25334], D_loss: 0.0029, G_loss: 22.8997\n",
      "Epoch [1/5], Step [946/25334], D_loss: 0.0083, G_loss: 8.7318\n",
      "Epoch [1/5], Step [947/25334], D_loss: 0.0064, G_loss: 14.7643\n",
      "Epoch [1/5], Step [948/25334], D_loss: 0.0031, G_loss: 14.7895\n",
      "Epoch [1/5], Step [949/25334], D_loss: 0.0046, G_loss: 11.9823\n",
      "Epoch [1/5], Step [950/25334], D_loss: 0.0808, G_loss: 9.0445\n",
      "Epoch [1/5], Step [951/25334], D_loss: 0.0415, G_loss: 7.7780\n",
      "Epoch [1/5], Step [952/25334], D_loss: 0.0038, G_loss: 11.0958\n",
      "Epoch [1/5], Step [953/25334], D_loss: 0.0997, G_loss: 7.5496\n",
      "Epoch [1/5], Step [954/25334], D_loss: 0.0034, G_loss: 14.5175\n",
      "Epoch [1/5], Step [955/25334], D_loss: 0.0047, G_loss: 11.0211\n",
      "Epoch [1/5], Step [956/25334], D_loss: 0.1151, G_loss: 5.5553\n",
      "Epoch [1/5], Step [957/25334], D_loss: 0.0031, G_loss: 20.9928\n",
      "Epoch [1/5], Step [958/25334], D_loss: 0.0049, G_loss: 16.9093\n",
      "Epoch [1/5], Step [959/25334], D_loss: 0.0063, G_loss: 14.4427\n",
      "Epoch [1/5], Step [960/25334], D_loss: 0.0056, G_loss: 13.1000\n",
      "Epoch [1/5], Step [961/25334], D_loss: 0.0113, G_loss: 10.8006\n",
      "Epoch [1/5], Step [962/25334], D_loss: 0.0144, G_loss: 12.1550\n",
      "Epoch [1/5], Step [963/25334], D_loss: 0.3379, G_loss: 6.5436\n",
      "Epoch [1/5], Step [964/25334], D_loss: 0.0192, G_loss: 53.7945\n",
      "Epoch [1/5], Step [965/25334], D_loss: 0.5861, G_loss: 9.0388\n",
      "Epoch [1/5], Step [966/25334], D_loss: 0.1833, G_loss: 5.7401\n",
      "Epoch [1/5], Step [967/25334], D_loss: 0.0087, G_loss: 15.6207\n",
      "Epoch [1/5], Step [968/25334], D_loss: 0.5809, G_loss: 7.5196\n",
      "Epoch [1/5], Step [969/25334], D_loss: 0.2544, G_loss: 8.9412\n",
      "Epoch [1/5], Step [970/25334], D_loss: 0.0067, G_loss: 12.0331\n",
      "Epoch [1/5], Step [971/25334], D_loss: 0.0277, G_loss: 8.6670\n",
      "Epoch [1/5], Step [972/25334], D_loss: 0.0132, G_loss: 14.4624\n",
      "Epoch [1/5], Step [973/25334], D_loss: 0.0130, G_loss: 7.4215\n",
      "Epoch [1/5], Step [974/25334], D_loss: 0.2263, G_loss: 6.6201\n",
      "Epoch [1/5], Step [975/25334], D_loss: 0.0434, G_loss: 5.8856\n",
      "Epoch [1/5], Step [976/25334], D_loss: 0.0164, G_loss: 20.6076\n",
      "Epoch [1/5], Step [977/25334], D_loss: 0.0691, G_loss: 7.1598\n",
      "Epoch [1/5], Step [978/25334], D_loss: 0.0671, G_loss: 6.6500\n",
      "Epoch [1/5], Step [979/25334], D_loss: 0.0148, G_loss: 8.7911\n",
      "Epoch [1/5], Step [980/25334], D_loss: 0.0173, G_loss: 21.8427\n",
      "Epoch [1/5], Step [981/25334], D_loss: 0.0121, G_loss: 9.3420\n",
      "Epoch [1/5], Step [982/25334], D_loss: 0.1790, G_loss: 5.9336\n",
      "Epoch [1/5], Step [983/25334], D_loss: 0.2118, G_loss: 12.8056\n",
      "Epoch [1/5], Step [984/25334], D_loss: 0.0078, G_loss: 7.8442\n",
      "Epoch [1/5], Step [985/25334], D_loss: 0.0494, G_loss: 5.6623\n",
      "Epoch [1/5], Step [986/25334], D_loss: 0.0155, G_loss: 12.2736\n",
      "Epoch [1/5], Step [987/25334], D_loss: 0.0200, G_loss: 10.3761\n",
      "Epoch [1/5], Step [988/25334], D_loss: 0.0095, G_loss: 11.1529\n",
      "Epoch [1/5], Step [989/25334], D_loss: 0.2255, G_loss: 5.8611\n",
      "Epoch [1/5], Step [990/25334], D_loss: 0.0660, G_loss: 10.0976\n",
      "Epoch [1/5], Step [991/25334], D_loss: 0.0311, G_loss: 15.1546\n",
      "Epoch [1/5], Step [992/25334], D_loss: 0.0059, G_loss: 19.0093\n",
      "Epoch [1/5], Step [993/25334], D_loss: 0.0708, G_loss: 8.9461\n",
      "Epoch [1/5], Step [994/25334], D_loss: 0.0035, G_loss: 18.6477\n",
      "Epoch [1/5], Step [995/25334], D_loss: 0.0576, G_loss: 6.4759\n",
      "Epoch [1/5], Step [996/25334], D_loss: 0.0098, G_loss: 9.3044\n",
      "Epoch [1/5], Step [997/25334], D_loss: 0.0090, G_loss: 7.7809\n",
      "Epoch [1/5], Step [998/25334], D_loss: 0.0089, G_loss: 9.8503\n",
      "Epoch [1/5], Step [999/25334], D_loss: 0.2152, G_loss: 3.9421\n",
      "Epoch [1/5], Step [1000/25334], D_loss: 0.0125, G_loss: 31.0414\n",
      "Epoch [1/5], Step [1001/25334], D_loss: 0.1441, G_loss: 11.0923\n",
      "Epoch [1/5], Step [1002/25334], D_loss: 0.0088, G_loss: 9.8347\n",
      "Epoch [1/5], Step [1003/25334], D_loss: 0.1141, G_loss: 7.6835\n",
      "Epoch [1/5], Step [1004/25334], D_loss: 0.0093, G_loss: 11.2535\n",
      "Epoch [1/5], Step [1005/25334], D_loss: 0.0164, G_loss: 8.7208\n",
      "Epoch [1/5], Step [1006/25334], D_loss: 0.0493, G_loss: 7.4799\n",
      "Epoch [1/5], Step [1007/25334], D_loss: 0.0122, G_loss: 18.0314\n",
      "Epoch [1/5], Step [1008/25334], D_loss: 0.2223, G_loss: 7.6890\n",
      "Epoch [1/5], Step [1009/25334], D_loss: 0.0702, G_loss: 8.4980\n",
      "Epoch [1/5], Step [1010/25334], D_loss: 0.0209, G_loss: 16.0319\n",
      "Epoch [1/5], Step [1011/25334], D_loss: 0.0287, G_loss: 11.1589\n",
      "Epoch [1/5], Step [1012/25334], D_loss: 0.0665, G_loss: 5.1069\n",
      "Epoch [1/5], Step [1013/25334], D_loss: 0.0085, G_loss: 14.6291\n",
      "Epoch [1/5], Step [1014/25334], D_loss: 0.0069, G_loss: 10.6218\n",
      "Epoch [1/5], Step [1015/25334], D_loss: 0.0121, G_loss: 15.0943\n",
      "Epoch [1/5], Step [1016/25334], D_loss: 0.0151, G_loss: 27.4236\n",
      "Epoch [1/5], Step [1017/25334], D_loss: 0.0060, G_loss: 14.7830\n",
      "Epoch [1/5], Step [1018/25334], D_loss: 0.0097, G_loss: 12.9026\n",
      "Epoch [1/5], Step [1019/25334], D_loss: 0.0076, G_loss: 10.3285\n",
      "Epoch [1/5], Step [1020/25334], D_loss: 0.0091, G_loss: 8.7281\n",
      "Epoch [1/5], Step [1021/25334], D_loss: 0.0387, G_loss: 22.1788\n",
      "Epoch [1/5], Step [1022/25334], D_loss: 0.0767, G_loss: 8.0968\n",
      "Epoch [1/5], Step [1023/25334], D_loss: 0.0239, G_loss: 11.4749\n",
      "Epoch [1/5], Step [1024/25334], D_loss: 0.0130, G_loss: 22.8758\n",
      "Epoch [1/5], Step [1025/25334], D_loss: 0.1157, G_loss: 7.7601\n",
      "Epoch [1/5], Step [1026/25334], D_loss: 0.0052, G_loss: 12.9522\n",
      "Epoch [1/5], Step [1027/25334], D_loss: 0.0085, G_loss: 13.4837\n",
      "Epoch [1/5], Step [1028/25334], D_loss: 0.0073, G_loss: 17.3153\n",
      "Epoch [1/5], Step [1029/25334], D_loss: 0.0167, G_loss: 8.5667\n",
      "Epoch [1/5], Step [1030/25334], D_loss: 0.0111, G_loss: 7.5051\n",
      "Epoch [1/5], Step [1031/25334], D_loss: 0.0092, G_loss: 13.1879\n",
      "Epoch [1/5], Step [1032/25334], D_loss: 0.0049, G_loss: 14.6894\n",
      "Epoch [1/5], Step [1033/25334], D_loss: 0.0046, G_loss: 17.6668\n",
      "Epoch [1/5], Step [1034/25334], D_loss: 0.0036, G_loss: 11.0190\n",
      "Epoch [1/5], Step [1035/25334], D_loss: 0.0103, G_loss: 8.2465\n",
      "Epoch [1/5], Step [1036/25334], D_loss: 0.0029, G_loss: 16.0849\n",
      "Epoch [1/5], Step [1037/25334], D_loss: 0.1083, G_loss: 6.8153\n",
      "Epoch [1/5], Step [1038/25334], D_loss: 0.0195, G_loss: 6.7899\n",
      "Epoch [1/5], Step [1039/25334], D_loss: 0.0054, G_loss: 20.0514\n",
      "Epoch [1/5], Step [1040/25334], D_loss: 0.0036, G_loss: 12.7773\n",
      "Epoch [1/5], Step [1041/25334], D_loss: 0.0038, G_loss: 19.0883\n",
      "Epoch [1/5], Step [1042/25334], D_loss: 0.0043, G_loss: 16.5785\n",
      "Epoch [1/5], Step [1043/25334], D_loss: 0.1388, G_loss: 4.9085\n",
      "Epoch [1/5], Step [1044/25334], D_loss: 0.0072, G_loss: 7.5802\n",
      "Epoch [1/5], Step [1045/25334], D_loss: 0.0076, G_loss: 16.1997\n",
      "Epoch [1/5], Step [1046/25334], D_loss: 0.0059, G_loss: 12.2263\n",
      "Epoch [1/5], Step [1047/25334], D_loss: 0.0070, G_loss: 31.2275\n",
      "Epoch [1/5], Step [1048/25334], D_loss: 0.0043, G_loss: 12.6146\n",
      "Epoch [1/5], Step [1049/25334], D_loss: 0.0059, G_loss: 11.3662\n",
      "Epoch [1/5], Step [1050/25334], D_loss: 0.0338, G_loss: 7.1487\n",
      "Epoch [1/5], Step [1051/25334], D_loss: 0.0026, G_loss: 12.9140\n",
      "Epoch [1/5], Step [1052/25334], D_loss: 0.0073, G_loss: 10.2818\n",
      "Epoch [1/5], Step [1053/25334], D_loss: 0.0153, G_loss: 8.4290\n",
      "Epoch [1/5], Step [1054/25334], D_loss: 0.3409, G_loss: 4.9970\n",
      "Epoch [1/5], Step [1055/25334], D_loss: 0.0040, G_loss: 11.3171\n",
      "Epoch [1/5], Step [1056/25334], D_loss: 0.0063, G_loss: 18.6658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1057/25334], D_loss: 0.0073, G_loss: 17.5457\n",
      "Epoch [1/5], Step [1058/25334], D_loss: 0.0097, G_loss: 8.3540\n",
      "Epoch [1/5], Step [1059/25334], D_loss: 0.0218, G_loss: 5.7741\n",
      "Epoch [1/5], Step [1060/25334], D_loss: 0.0143, G_loss: 21.4366\n",
      "Epoch [1/5], Step [1061/25334], D_loss: 0.0136, G_loss: 11.8310\n",
      "Epoch [1/5], Step [1062/25334], D_loss: 0.0666, G_loss: 10.9619\n",
      "Epoch [1/5], Step [1063/25334], D_loss: 0.0033, G_loss: 20.2923\n",
      "Epoch [1/5], Step [1064/25334], D_loss: 0.0053, G_loss: 15.7082\n",
      "Epoch [1/5], Step [1065/25334], D_loss: 0.2916, G_loss: 3.3099\n",
      "Epoch [1/5], Step [1066/25334], D_loss: 0.0434, G_loss: 8.5029\n",
      "Epoch [1/5], Step [1067/25334], D_loss: 0.2472, G_loss: 6.8631\n",
      "Epoch [1/5], Step [1068/25334], D_loss: 0.1950, G_loss: 15.1086\n",
      "Epoch [1/5], Step [1069/25334], D_loss: 0.3709, G_loss: 6.8670\n",
      "Epoch [1/5], Step [1070/25334], D_loss: 0.0300, G_loss: 12.2841\n",
      "Epoch [1/5], Step [1071/25334], D_loss: 0.0452, G_loss: 11.6085\n",
      "Epoch [1/5], Step [1072/25334], D_loss: 0.2170, G_loss: 36.2574\n",
      "Epoch [1/5], Step [1073/25334], D_loss: 0.1010, G_loss: 22.0766\n",
      "Epoch [1/5], Step [1074/25334], D_loss: 0.1948, G_loss: 8.8483\n",
      "Epoch [1/5], Step [1075/25334], D_loss: 0.0297, G_loss: 9.8868\n",
      "Epoch [1/5], Step [1076/25334], D_loss: 0.0119, G_loss: 17.4001\n",
      "Epoch [1/5], Step [1077/25334], D_loss: 0.0236, G_loss: 10.8383\n",
      "Epoch [1/5], Step [1078/25334], D_loss: 0.0408, G_loss: 9.5039\n",
      "Epoch [1/5], Step [1079/25334], D_loss: 0.1258, G_loss: 6.9819\n",
      "Epoch [1/5], Step [1080/25334], D_loss: 0.0339, G_loss: 8.6706\n",
      "Epoch [1/5], Step [1081/25334], D_loss: 0.0164, G_loss: 16.0261\n",
      "Epoch [1/5], Step [1082/25334], D_loss: 0.0049, G_loss: 11.6957\n",
      "Epoch [1/5], Step [1083/25334], D_loss: 0.4452, G_loss: 10.6629\n",
      "Epoch [1/5], Step [1084/25334], D_loss: 0.0976, G_loss: 15.7896\n",
      "Epoch [1/5], Step [1085/25334], D_loss: 0.3161, G_loss: 10.7328\n",
      "Epoch [1/5], Step [1086/25334], D_loss: 0.2765, G_loss: 6.3986\n",
      "Epoch [1/5], Step [1087/25334], D_loss: 0.0197, G_loss: 15.7523\n",
      "Epoch [1/5], Step [1088/25334], D_loss: 0.0414, G_loss: 5.8869\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3b6dda862483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         x_ = torch.stft(input.squeeze(), n_fft = 255, win_length = 255, window = torch.hann_window(window_length = 255),\n\u001b[1;32m     11\u001b[0m           hop_length=125).unsqueeze(0).reshape((1, 2, 128, 128)).cuda()\n\u001b[0;32m---> 12\u001b[0;31m         y_ = torch.stft(target.squeeze(), n_fft = 255, win_length = 255, window = torch.hann_window(window_length = 255),\n\u001b[0m\u001b[1;32m     13\u001b[0m           hop_length=125).unsqueeze(0).reshape((1, 2, 128, 128)).cuda()\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(x_.size(), y_.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kalditorch/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msignal_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monesided\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(5):\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "\n",
    "    # training\n",
    "    for i, (target, input, _) in enumerate(train_data):\n",
    "\n",
    "        # input & target image data\n",
    "        x_ = torch.stft(input.squeeze(), n_fft = 255, win_length = 255, window = torch.hann_window(window_length = 255),\n",
    "          hop_length=125).unsqueeze(0).reshape((1, 2, 128, 128)).cuda()\n",
    "        y_ = torch.stft(target.squeeze(), n_fft = 255, win_length = 255, window = torch.hann_window(window_length = 255),\n",
    "          hop_length=125).unsqueeze(0).reshape((1, 2, 128, 128)).cuda()\n",
    "        #print(x_.size(), y_.size())\n",
    "\n",
    "        # Train discriminator with real data\n",
    "        D_real_decision = D(x_, y_).squeeze()\n",
    "        real_ = torch.ones(D_real_decision.size()).cuda()\n",
    "        D_real_loss = BCE_loss(D_real_decision, real_)\n",
    "\n",
    "        # Train discriminator with fake data\n",
    "        gen_image = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        fake_ = torch.zeros(D_fake_decision.size()).cuda()\n",
    "        D_fake_loss = BCE_loss(D_fake_decision, fake_)\n",
    "\n",
    "        # Back propagation\n",
    "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        gen_image = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        G_fake_loss = BCE_loss(D_fake_decision, real_)\n",
    "\n",
    "        # L1 loss\n",
    "        l1_loss = 100 * L1_loss(gen_image, y_)\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_fake_loss + l1_loss\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # loss values\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "\n",
    "        print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "              % (epoch+1, 5, i+1, len(train_data), D_loss.item(), G_loss.item()))\n",
    "\n",
    "        # ============ TensorBoard logging ============#\n",
    "#         D_logger.scalar_summary('losses', D_loss.data[0], step + 1)\n",
    "#         G_logger.scalar_summary('losses', G_loss.data[0], step + 1)\n",
    "#         step += 1\n",
    "\n",
    "    D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
    "    G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_avg_losses.append(D_avg_loss)\n",
    "    G_avg_losses.append(G_avg_loss)\n",
    "\n",
    "    # Show result for test image\n",
    "#     gen_image = G(Variable(test_input.cuda()))\n",
    "#     gen_image = gen_image.cpu().data\n",
    "#     utils.plot_test_result(test_input, test_target, gen_image, epoch, save=True, save_dir=save_dir)\n",
    "\n",
    "# Plot average losses\n",
    "plot_loss(D_avg_losses, G_avg_losses, 5, save=False, save_dir=save_dir)\n",
    "\n",
    "# Make gif\n",
    "#utils.make_gif(params.dataset, params.num_epochs, save_dir=save_dir)\n",
    "\n",
    "# Save trained parameters of model\n",
    "# torch.save(G.state_dict(), model_dir + 'generator_param.pkl')\n",
    "# torch.save(D.state_dict(), model_dir + 'discriminator_param.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(2, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): ConvBlock(\n",
       "    (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv4): ConvBlock(\n",
       "    (conv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv5): ConvBlock(\n",
       "    (conv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv6): ConvBlock(\n",
       "    (conv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv7): ConvBlock(\n",
       "    (conv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (deconv2): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv3): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv4): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv5): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(2048, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv6): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv7): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv8): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(256, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = torch.rand((1, 2, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 16, 16])\n",
      "torch.Size([1, 1024, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 128, 128])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(prova).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class ResidualConv1dGLU(nn.Module):\n",
    "    \"\"\"Residual dilated conv1d + Gated linear unit\n",
    "    Args:\n",
    "        residual_channels (int): Residual input / output channels\n",
    "        gate_channels (int): Gated activation channels.\n",
    "        kernel_size (int): Kernel size of convolution layers.\n",
    "        skip_out_channels (int): Skip connection channels. If None, set to same\n",
    "          as ``residual_channels``.\n",
    "        cin_channels (int): Local conditioning channels. If negative value is\n",
    "          set, local conditioning is disabled.\n",
    "        gin_channels (int): Global conditioning channels. If negative value is\n",
    "          set, global conditioning is disabled.\n",
    "        dropout (float): Dropout probability.\n",
    "        padding (int): Padding for convolution layers. If None, proper padding\n",
    "          is computed depends on dilation and kernel_size.\n",
    "        dilation (int): Dilation factor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, residual_channels, gate_channels, kernel_size,\n",
    "                 skip_out_channels=None,\n",
    "                 cin_channels=-1, gin_channels=-1,\n",
    "                 dropout=1 - 0.95, padding=None, dilation=1,\n",
    "                 bias=True, *args, **kwargs):\n",
    "        super(ResidualConv1dGLU, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        if skip_out_channels is None:\n",
    "            skip_out_channels = residual_channels\n",
    "        if padding is None:\n",
    "            padding = (kernel_size - 1) // 2 * dilation\n",
    "\n",
    "        self.conv = nn.Conv1d(residual_channels, gate_channels, kernel_size,\n",
    "                              padding=padding, dilation=dilation,\n",
    "                              bias=bias, *args, **kwargs)\n",
    "\n",
    "        # local conditioning\n",
    "        if cin_channels > 0:\n",
    "            self.conv1x1c = nn.Conv1d(cin_channels, gate_channels, 1, bias=bias)\n",
    "        else:\n",
    "            self.conv1x1c = None\n",
    "\n",
    "        # global conditioning\n",
    "        if gin_channels > 0:\n",
    "            self.conv1x1g = nn.Conv1d(gin_channels, gate_channels, 1, bias=bias)\n",
    "\n",
    "        else:\n",
    "            self.conv1x1g = None\n",
    "\n",
    "        # conv output is split into two groups\n",
    "        gate_out_channels = gate_channels // 2\n",
    "        self.conv1x1_out = nn.Conv1d(gate_out_channels, residual_channels, 1, bias=bias)\n",
    "        self.conv1x1_skip = nn.Conv1d(gate_out_channels, skip_out_channels, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x, c=None, g=None):\n",
    "        \"\"\"Forward\n",
    "        Args:\n",
    "            x (Tensor): B x C x T\n",
    "            c (Tensor): B x C x T, Local conditioning features\n",
    "            g (Tensor): B x C x T, Expanded global conditioning features\n",
    "        Returns:\n",
    "            Tensor: output\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        splitdim = 1\n",
    "        x = self.conv(x)\n",
    "\n",
    "        a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n",
    "\n",
    "        # local conditioning\n",
    "        if c is not None:\n",
    "            assert self.conv1x1c is not None\n",
    "            c = self.conv1x1c(c)\n",
    "            ca, cb = c.split(c.size(splitdim) // 2, dim=splitdim)\n",
    "            a, b = a + ca, b + cb\n",
    "\n",
    "        # global conditioning\n",
    "        if g is not None:\n",
    "            assert self.conv1x1g is not None\n",
    "            g = self.conv1x1g(g)\n",
    "            ga, gb = g.split(g.size(splitdim) // 2, dim=splitdim)\n",
    "            a, b = a + ga, b + gb\n",
    "\n",
    "        x = torch.tanh(a) * torch.sigmoid(b)\n",
    "\n",
    "        # For skip connection\n",
    "        s = self.conv1x1_skip(x)\n",
    "\n",
    "        # For residual connection\n",
    "        x = self.conv1x1_out(x)\n",
    "\n",
    "        x = (x + residual) * math.sqrt(0.5)\n",
    "        return x, s\n",
    "\n",
    "\n",
    "class DWaveNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=1, bias=False,\n",
    "                 num_layers=30, num_stacks=3,\n",
    "                 kernel_size=3,\n",
    "                 residual_channels=128, gate_channels=128, skip_out_channels=128,\n",
    "                 last_channels=(2048, 256),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert num_layers % num_stacks == 0\n",
    "        num_layers_per_stack = num_layers // num_stacks\n",
    "        self.l_diff = num_stacks * (2**num_layers_per_stack - 1)\n",
    "\n",
    "        self.first_conv = nn.Conv1d(in_channels, residual_channels, 3, padding=1, bias=bias)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for n_layer in range(num_layers):\n",
    "            dilation = 2**(n_layer % num_layers_per_stack)\n",
    "            conv = ResidualConv1dGLU(\n",
    "                residual_channels, gate_channels,\n",
    "                skip_out_channels=skip_out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                bias=bias,\n",
    "                dilation=dilation,\n",
    "                dropout=1 - 0.95,\n",
    "            )\n",
    "            self.conv_layers.append(conv)\n",
    "\n",
    "        self.last_conv_layers = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(skip_out_channels, last_channels[0], 3, padding=1, bias=bias),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(last_channels[0], last_channels[1], 3, padding=1, bias=bias),\n",
    "            nn.Conv1d(last_channels[1], out_channels, 1, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        skips = None\n",
    "        for conv in self.conv_layers:\n",
    "            x, h = conv(x)\n",
    "            if skips is None:\n",
    "                skips = h#[..., self.l_diff:-self.l_diff]\n",
    "            else:\n",
    "                skips += h#[..., self.l_diff:-self.l_diff]\n",
    "\n",
    "        x = skips\n",
    "        x = self.last_conv_layers(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(16%10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13%10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DWaveNet(in_channels = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3069"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*(2**10 -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = torch.randn((1, 1, 16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8000])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16004) must match the size of tensor b (16000) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-591143f8dd36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprova\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kalditorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-55d191f8927f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskips\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;31m#[..., self.l_diff:-self.l_diff]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kalditorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-55d191f8927f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c, g)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1x1_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16004) must match the size of tensor b (16000) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "net(prova).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = ResidualConv1dGLU()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
